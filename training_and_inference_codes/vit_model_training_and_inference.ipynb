{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:15:10.683002Z",
     "iopub.status.busy": "2024-11-13T06:15:10.682614Z",
     "iopub.status.idle": "2024-11-13T06:15:11.027221Z",
     "shell.execute_reply": "2024-11-13T06:15:11.026431Z",
     "shell.execute_reply.started": "2024-11-13T06:15:10.682966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:15:15.467519Z",
     "iopub.status.busy": "2024-11-13T06:15:15.467158Z",
     "iopub.status.idle": "2024-11-13T06:15:15.472089Z",
     "shell.execute_reply": "2024-11-13T06:15:15.471106Z",
     "shell.execute_reply.started": "2024-11-13T06:15:15.467484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_csv_path = '/kaggle/input/visual-taxonomy/train.csv'\n",
    "test_csv_path = '/kaggle/input/visual-taxonomy/test.csv'\n",
    "category_attributes_path = '/kaggle/input/visual-taxonomy/category_attributes.parquet'\n",
    "sample_submission_path = '/kaggle/input/visual-taxonomy/sample_submission.csv'\n",
    "images_folder = '/kaggle/input/visual-taxonomy/train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:15:19.159606Z",
     "iopub.status.busy": "2024-11-13T06:15:19.159232Z",
     "iopub.status.idle": "2024-11-13T06:15:19.393495Z",
     "shell.execute_reply": "2024-11-13T06:15:19.392768Z",
     "shell.execute_reply.started": "2024-11-13T06:15:19.159571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train.csv ...\n",
      "train.csv shape: (70213, 13)\n",
      "train.csv columns: Index(['id', 'Category', 'len', 'attr_1', 'attr_2', 'attr_3', 'attr_4',\n",
      "       'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Category</th>\n",
       "      <th>len</th>\n",
       "      <th>attr_1</th>\n",
       "      <th>attr_2</th>\n",
       "      <th>attr_3</th>\n",
       "      <th>attr_4</th>\n",
       "      <th>attr_5</th>\n",
       "      <th>attr_6</th>\n",
       "      <th>attr_7</th>\n",
       "      <th>attr_8</th>\n",
       "      <th>attr_9</th>\n",
       "      <th>attr_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Men Tshirts</td>\n",
       "      <td>5</td>\n",
       "      <td>default</td>\n",
       "      <td>round</td>\n",
       "      <td>printed</td>\n",
       "      <td>default</td>\n",
       "      <td>short sleeves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Men Tshirts</td>\n",
       "      <td>5</td>\n",
       "      <td>multicolor</td>\n",
       "      <td>polo</td>\n",
       "      <td>solid</td>\n",
       "      <td>solid</td>\n",
       "      <td>short sleeves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Men Tshirts</td>\n",
       "      <td>5</td>\n",
       "      <td>default</td>\n",
       "      <td>polo</td>\n",
       "      <td>solid</td>\n",
       "      <td>solid</td>\n",
       "      <td>short sleeves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Men Tshirts</td>\n",
       "      <td>5</td>\n",
       "      <td>multicolor</td>\n",
       "      <td>polo</td>\n",
       "      <td>solid</td>\n",
       "      <td>solid</td>\n",
       "      <td>short sleeves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Men Tshirts</td>\n",
       "      <td>5</td>\n",
       "      <td>multicolor</td>\n",
       "      <td>polo</td>\n",
       "      <td>solid</td>\n",
       "      <td>solid</td>\n",
       "      <td>short sleeves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Category  len      attr_1 attr_2   attr_3   attr_4         attr_5  \\\n",
       "0   0  Men Tshirts    5     default  round  printed  default  short sleeves   \n",
       "1   1  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
       "2   2  Men Tshirts    5     default   polo    solid    solid  short sleeves   \n",
       "3   3  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
       "4   4  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
       "\n",
       "  attr_6 attr_7 attr_8 attr_9 attr_10  \n",
       "0    NaN    NaN    NaN    NaN     NaN  \n",
       "1    NaN    NaN    NaN    NaN     NaN  \n",
       "2    NaN    NaN    NaN    NaN     NaN  \n",
       "3    NaN    NaN    NaN    NaN     NaN  \n",
       "4    NaN    NaN    NaN    NaN     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoading train.csv ...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"train.csv shape: {train_df.shape}\")\n",
    "print(f\"train.csv columns: {train_df.columns}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T11:41:51.514021Z",
     "iopub.status.busy": "2024-11-10T11:41:51.513576Z",
     "iopub.status.idle": "2024-11-10T11:41:51.544871Z",
     "shell.execute_reply": "2024-11-10T11:41:51.543945Z",
     "shell.execute_reply.started": "2024-11-10T11:41:51.513975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test.csv ...\n",
      "test.csv shape: (30205, 2)\n",
      "test.csv columns: Index(['id', 'Category'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Men Tshirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Men Tshirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Men Tshirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Men Tshirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Men Tshirts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Category\n",
       "0   0  Men Tshirts\n",
       "1   1  Men Tshirts\n",
       "2   2  Men Tshirts\n",
       "3   3  Men Tshirts\n",
       "4   4  Men Tshirts"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoading test.csv ...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"test.csv shape: {test_df.shape}\")\n",
    "print(f\"test.csv columns: {test_df.columns}\")\n",
    "\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:13:35.633600Z",
     "iopub.status.busy": "2024-11-10T13:13:35.633211Z",
     "iopub.status.idle": "2024-11-10T13:13:35.822874Z",
     "shell.execute_reply": "2024-11-10T13:13:35.821963Z",
     "shell.execute_reply.started": "2024-11-10T13:13:35.633563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7267 entries, 0 to 7266\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7267 non-null   int64 \n",
      " 1   Category  7267 non-null   object\n",
      " 2   len       7267 non-null   int64 \n",
      " 3   attr_1    6010 non-null   object\n",
      " 4   attr_2    6144 non-null   object\n",
      " 5   attr_3    5791 non-null   object\n",
      " 6   attr_4    5949 non-null   object\n",
      " 7   attr_5    5977 non-null   object\n",
      " 8   attr_6    0 non-null      object\n",
      " 9   attr_7    0 non-null      object\n",
      " 10  attr_8    0 non-null      object\n",
      " 11  attr_9    0 non-null      object\n",
      " 12  attr_10   0 non-null      object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 794.8+ KB\n",
      "Men Tshirts DataFrame:\n",
      "None\n",
      "   id     Category  len      attr_1 attr_2   attr_3   attr_4         attr_5  \\\n",
      "0   0  Men Tshirts    5     default  round  printed  default  short sleeves   \n",
      "1   1  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
      "2   2  Men Tshirts    5     default   polo    solid    solid  short sleeves   \n",
      "3   3  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
      "4   4  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
      "\n",
      "  attr_6 attr_7 attr_8 attr_9 attr_10  \n",
      "0    NaN    NaN    NaN    NaN     NaN  \n",
      "1    NaN    NaN    NaN    NaN     NaN  \n",
      "2    NaN    NaN    NaN    NaN     NaN  \n",
      "3    NaN    NaN    NaN    NaN     NaN  \n",
      "4    NaN    NaN    NaN    NaN     NaN  \n",
      "7267\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18346 entries, 7267 to 25612\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        18346 non-null  int64 \n",
      " 1   Category  18346 non-null  object\n",
      " 2   len       18346 non-null  int64 \n",
      " 3   attr_1    7885 non-null   object\n",
      " 4   attr_2    17679 non-null  object\n",
      " 5   attr_3    15861 non-null  object\n",
      " 6   attr_4    17896 non-null  object\n",
      " 7   attr_5    17649 non-null  object\n",
      " 8   attr_6    5010 non-null   object\n",
      " 9   attr_7    8896 non-null   object\n",
      " 10  attr_8    16465 non-null  object\n",
      " 11  attr_9    14303 non-null  object\n",
      " 12  attr_10   17818 non-null  object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 2.0+ MB\n",
      "Sarees DataFrame:\n",
      "None\n",
      "        id Category  len         attr_1        attr_2        attr_3  \\\n",
      "7267  7432   Sarees   10  same as saree  woven design  small border   \n",
      "7268  7433   Sarees   10            NaN          zari  small border   \n",
      "7269  7434   Sarees   10            NaN          zari  small border   \n",
      "7270  7435   Sarees   10  same as saree  woven design    big border   \n",
      "7271  7436   Sarees   10          solid     no border           NaN   \n",
      "\n",
      "          attr_4       attr_5    attr_6         attr_7      attr_8  \\\n",
      "7267  multicolor        party  jacquard   woven design  zari woven   \n",
      "7268       cream  traditional       NaN            NaN  zari woven   \n",
      "7269       white        party       NaN            NaN  zari woven   \n",
      "7270     default  traditional       NaN  same as saree  zari woven   \n",
      "7271         NaN        daily       NaN            NaN         NaN   \n",
      "\n",
      "            attr_9 attr_10  \n",
      "7267      applique      no  \n",
      "7268      elephant      no  \n",
      "7269        floral      no  \n",
      "7270  ethnic motif      no  \n",
      "7271           NaN     yes  \n",
      "18346\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6822 entries, 25613 to 32434\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        6822 non-null   int64 \n",
      " 1   Category  6822 non-null   object\n",
      " 2   len       6822 non-null   int64 \n",
      " 3   attr_1    6629 non-null   object\n",
      " 4   attr_2    3231 non-null   object\n",
      " 5   attr_3    3400 non-null   object\n",
      " 6   attr_4    6431 non-null   object\n",
      " 7   attr_5    3266 non-null   object\n",
      " 8   attr_6    3848 non-null   object\n",
      " 9   attr_7    3843 non-null   object\n",
      " 10  attr_8    6702 non-null   object\n",
      " 11  attr_9    6691 non-null   object\n",
      " 12  attr_10   0 non-null      object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 746.2+ KB\n",
      "Kurtis DataFrame:\n",
      "None\n",
      "          id Category  len     attr_1    attr_2       attr_3 attr_4   attr_5  \\\n",
      "25613  25778   Kurtis    9      black  straight  knee length  daily      net   \n",
      "25614  25779   Kurtis    9        red  straight  knee length  daily  default   \n",
      "25615  25780   Kurtis    9        red  straight  knee length  daily  default   \n",
      "25616  25781   Kurtis    9  navy blue  straight  knee length  daily  default   \n",
      "25617  25782   Kurtis    9      black  straight  knee length  daily  default   \n",
      "\n",
      "        attr_6   attr_7                 attr_8   attr_9 attr_10  \n",
      "25613    solid    solid  three-quarter sleeves  regular     NaN  \n",
      "25614  default  default  three-quarter sleeves  regular     NaN  \n",
      "25615  default  default  three-quarter sleeves  regular     NaN  \n",
      "25616  default  default  three-quarter sleeves  regular     NaN  \n",
      "25617  default  default  three-quarter sleeves  regular     NaN  \n",
      "6822\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18774 entries, 32435 to 51208\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        18774 non-null  int64 \n",
      " 1   Category  18774 non-null  object\n",
      " 2   len       18774 non-null  int64 \n",
      " 3   attr_1    17285 non-null  object\n",
      " 4   attr_2    14801 non-null  object\n",
      " 5   attr_3    16580 non-null  object\n",
      " 6   attr_4    16161 non-null  object\n",
      " 7   attr_5    17034 non-null  object\n",
      " 8   attr_6    16041 non-null  object\n",
      " 9   attr_7    15460 non-null  object\n",
      " 10  attr_8    510 non-null    object\n",
      " 11  attr_9    0 non-null      object\n",
      " 12  attr_10   0 non-null      object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 2.0+ MB\n",
      "Women Tshirts:\n",
      "None\n",
      "          id       Category  len      attr_1 attr_2 attr_3   attr_4   attr_5  \\\n",
      "32435  32601  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "32436  32602  Women Tshirts    8      yellow  loose   long  default  default   \n",
      "32437  32603  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "32438  32604  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "32439  32605  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "\n",
      "             attr_6           attr_7 attr_8 attr_9 attr_10  \n",
      "32435       default  regular sleeves    NaN    NaN     NaN  \n",
      "32436  long sleeves  regular sleeves    NaN    NaN     NaN  \n",
      "32437       default  regular sleeves    NaN    NaN     NaN  \n",
      "32438  long sleeves  regular sleeves    NaN    NaN     NaN  \n",
      "32439       default  regular sleeves    NaN    NaN     NaN  \n",
      "18774\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19004 entries, 51209 to 70212\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        19004 non-null  int64 \n",
      " 1   Category  19004 non-null  object\n",
      " 2   len       19004 non-null  int64 \n",
      " 3   attr_1    14058 non-null  object\n",
      " 4   attr_2    13337 non-null  object\n",
      " 5   attr_3    13066 non-null  object\n",
      " 6   attr_4    13451 non-null  object\n",
      " 7   attr_5    12567 non-null  object\n",
      " 8   attr_6    13217 non-null  object\n",
      " 9   attr_7    13216 non-null  object\n",
      " 10  attr_8    13797 non-null  object\n",
      " 11  attr_9    12571 non-null  object\n",
      " 12  attr_10   7181 non-null   object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 2.0+ MB\n",
      "Women Tops and Tunics DataFrame:\n",
      "None\n",
      "          id       Category  len      attr_1 attr_2 attr_3   attr_4   attr_5  \\\n",
      "32435  32601  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "32436  32602  Women Tshirts    8      yellow  loose   long  default  default   \n",
      "32437  32603  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "32438  32604  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "32439  32605  Women Tshirts    8  multicolor  loose   long  default  default   \n",
      "\n",
      "             attr_6           attr_7 attr_8 attr_9 attr_10  \n",
      "32435       default  regular sleeves    NaN    NaN     NaN  \n",
      "32436  long sleeves  regular sleeves    NaN    NaN     NaN  \n",
      "32437       default  regular sleeves    NaN    NaN     NaN  \n",
      "32438  long sleeves  regular sleeves    NaN    NaN     NaN  \n",
      "32439       default  regular sleeves    NaN    NaN     NaN  \n",
      "18774\n"
     ]
    }
   ],
   "source": [
    "category_dfs = {}\n",
    "\n",
    "for category in train_df['Category'].unique():\n",
    "    category_dfs[category] = train_df[train_df['Category'] == category]\n",
    "\n",
    "men_tshirts_df = category_dfs['Men Tshirts']\n",
    "print(f\"Men Tshirts DataFrame:\\n{men_tshirts_df.info()}\")\n",
    "print(men_tshirts_df.head())\n",
    "print(len(men_tshirts_df))\n",
    "\n",
    "sarees_df = category_dfs['Sarees']\n",
    "print(f\"Sarees DataFrame:\\n{sarees_df.info()}\")\n",
    "print(sarees_df.head())\n",
    "print(len(sarees_df))\n",
    "\n",
    "kurtis_df = category_dfs['Kurtis']\n",
    "print(f\"Kurtis DataFrame:\\n{kurtis_df.info()}\")\n",
    "print(kurtis_df.head())\n",
    "print(len(kurtis_df))\n",
    "\n",
    "women_tshirts_df = category_dfs['Women Tshirts']\n",
    "print(f\"Women Tshirts:\\n{women_tshirts_df.info()}\")\n",
    "print(women_tshirts_df.head())\n",
    "print(len(women_tshirts_df))\n",
    "\n",
    "women_tops_df = category_dfs['Women Tops & Tunics']\n",
    "print(f\"Women Tops and Tunics DataFrame:\\n{women_tops_df.info()}\")\n",
    "print(women_tshirts_df.head())\n",
    "print(len(women_tshirts_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:16:23.120778Z",
     "iopub.status.busy": "2024-11-13T06:16:23.120088Z",
     "iopub.status.idle": "2024-11-13T06:16:39.686006Z",
     "shell.execute_reply": "2024-11-13T06:16:39.685151Z",
     "shell.execute_reply.started": "2024-11-13T06:16:23.120739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T11:42:29.948201Z",
     "iopub.status.busy": "2024-11-10T11:42:29.947460Z",
     "iopub.status.idle": "2024-11-10T11:42:29.953370Z",
     "shell.execute_reply": "2024-11-10T11:42:29.952509Z",
     "shell.execute_reply.started": "2024-11-10T11:42:29.948156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing function for images\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    return tf.keras.applications.resnet.preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T11:49:07.848943Z",
     "iopub.status.busy": "2024-11-10T11:49:07.848566Z",
     "iopub.status.idle": "2024-11-10T12:12:54.527087Z",
     "shell.execute_reply": "2024-11-10T12:12:54.525972Z",
     "shell.execute_reply.started": "2024-11-10T11:49:07.848881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/3288356590.py:132: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Training Epoch 1/10:   0%|          | 0/402 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_30/3288356590.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1/10: 100%|██████████| 402/402 [01:47<00:00,  4.33it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Attribute attr_1, Validation Accuracy: 0.8648\n",
      "Epoch 1, Attribute attr_2, Validation Accuracy: 0.6770\n",
      "Epoch 1, Attribute attr_3, Validation Accuracy: 0.8438\n",
      "Epoch 1, Attribute attr_4, Validation Accuracy: 0.5509\n",
      "Epoch 1, Attribute attr_5, Validation Accuracy: 0.6962\n",
      "Epoch 1, Attribute attr_6, Validation Accuracy: 0.9564\n",
      "Epoch 1, Attribute attr_7, Validation Accuracy: 0.7224\n",
      "Epoch 1, Attribute attr_8, Validation Accuracy: 0.8630\n",
      "Epoch 1, Attribute attr_9, Validation Accuracy: 0.6472\n",
      "Epoch 1, Attribute attr_10, Validation Accuracy: 0.8343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Attribute attr_1, Validation Accuracy: 0.8594\n",
      "Epoch 2, Attribute attr_2, Validation Accuracy: 0.6733\n",
      "Epoch 2, Attribute attr_3, Validation Accuracy: 0.8448\n",
      "Epoch 2, Attribute attr_4, Validation Accuracy: 0.5523\n",
      "Epoch 2, Attribute attr_5, Validation Accuracy: 0.7078\n",
      "Epoch 2, Attribute attr_6, Validation Accuracy: 0.9578\n",
      "Epoch 2, Attribute attr_7, Validation Accuracy: 0.7260\n",
      "Epoch 2, Attribute attr_8, Validation Accuracy: 0.8532\n",
      "Epoch 2, Attribute attr_9, Validation Accuracy: 0.6395\n",
      "Epoch 2, Attribute attr_10, Validation Accuracy: 0.8339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Attribute attr_1, Validation Accuracy: 0.8656\n",
      "Epoch 3, Attribute attr_2, Validation Accuracy: 0.6831\n",
      "Epoch 3, Attribute attr_3, Validation Accuracy: 0.8470\n",
      "Epoch 3, Attribute attr_4, Validation Accuracy: 0.5436\n",
      "Epoch 3, Attribute attr_5, Validation Accuracy: 0.7035\n",
      "Epoch 3, Attribute attr_6, Validation Accuracy: 0.9549\n",
      "Epoch 3, Attribute attr_7, Validation Accuracy: 0.7384\n",
      "Epoch 3, Attribute attr_8, Validation Accuracy: 0.8674\n",
      "Epoch 3, Attribute attr_9, Validation Accuracy: 0.6541\n",
      "Epoch 3, Attribute attr_10, Validation Accuracy: 0.8343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Attribute attr_1, Validation Accuracy: 0.8637\n",
      "Epoch 4, Attribute attr_2, Validation Accuracy: 0.6897\n",
      "Epoch 4, Attribute attr_3, Validation Accuracy: 0.8361\n",
      "Epoch 4, Attribute attr_4, Validation Accuracy: 0.5632\n",
      "Epoch 4, Attribute attr_5, Validation Accuracy: 0.7071\n",
      "Epoch 4, Attribute attr_6, Validation Accuracy: 0.9589\n",
      "Epoch 4, Attribute attr_7, Validation Accuracy: 0.7387\n",
      "Epoch 4, Attribute attr_8, Validation Accuracy: 0.8619\n",
      "Epoch 4, Attribute attr_9, Validation Accuracy: 0.6544\n",
      "Epoch 4, Attribute attr_10, Validation Accuracy: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attribute attr_1, Validation Accuracy: 0.8641\n",
      "Epoch 5, Attribute attr_2, Validation Accuracy: 0.6857\n",
      "Epoch 5, Attribute attr_3, Validation Accuracy: 0.8496\n",
      "Epoch 5, Attribute attr_4, Validation Accuracy: 0.5552\n",
      "Epoch 5, Attribute attr_5, Validation Accuracy: 0.7133\n",
      "Epoch 5, Attribute attr_6, Validation Accuracy: 0.9568\n",
      "Epoch 5, Attribute attr_7, Validation Accuracy: 0.7402\n",
      "Epoch 5, Attribute attr_8, Validation Accuracy: 0.8648\n",
      "Epoch 5, Attribute attr_9, Validation Accuracy: 0.6548\n",
      "Epoch 5, Attribute attr_10, Validation Accuracy: 0.8358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Attribute attr_1, Validation Accuracy: 0.8634\n",
      "Epoch 6, Attribute attr_2, Validation Accuracy: 0.6897\n",
      "Epoch 6, Attribute attr_3, Validation Accuracy: 0.8467\n",
      "Epoch 6, Attribute attr_4, Validation Accuracy: 0.5531\n",
      "Epoch 6, Attribute attr_5, Validation Accuracy: 0.7148\n",
      "Epoch 6, Attribute attr_6, Validation Accuracy: 0.9575\n",
      "Epoch 6, Attribute attr_7, Validation Accuracy: 0.7253\n",
      "Epoch 6, Attribute attr_8, Validation Accuracy: 0.8557\n",
      "Epoch 6, Attribute attr_9, Validation Accuracy: 0.6537\n",
      "Epoch 6, Attribute attr_10, Validation Accuracy: 0.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Attribute attr_1, Validation Accuracy: 0.8656\n",
      "Epoch 7, Attribute attr_2, Validation Accuracy: 0.6926\n",
      "Epoch 7, Attribute attr_3, Validation Accuracy: 0.8474\n",
      "Epoch 7, Attribute attr_4, Validation Accuracy: 0.5600\n",
      "Epoch 7, Attribute attr_5, Validation Accuracy: 0.7184\n",
      "Epoch 7, Attribute attr_6, Validation Accuracy: 0.9586\n",
      "Epoch 7, Attribute attr_7, Validation Accuracy: 0.7449\n",
      "Epoch 7, Attribute attr_8, Validation Accuracy: 0.8601\n",
      "Epoch 7, Attribute attr_9, Validation Accuracy: 0.6621\n",
      "Epoch 7, Attribute attr_10, Validation Accuracy: 0.8347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Attribute attr_1, Validation Accuracy: 0.8659\n",
      "Epoch 8, Attribute attr_2, Validation Accuracy: 0.6919\n",
      "Epoch 8, Attribute attr_3, Validation Accuracy: 0.8474\n",
      "Epoch 8, Attribute attr_4, Validation Accuracy: 0.5636\n",
      "Epoch 8, Attribute attr_5, Validation Accuracy: 0.7177\n",
      "Epoch 8, Attribute attr_6, Validation Accuracy: 0.9593\n",
      "Epoch 8, Attribute attr_7, Validation Accuracy: 0.7435\n",
      "Epoch 8, Attribute attr_8, Validation Accuracy: 0.8685\n",
      "Epoch 8, Attribute attr_9, Validation Accuracy: 0.6613\n",
      "Epoch 8, Attribute attr_10, Validation Accuracy: 0.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Attribute attr_1, Validation Accuracy: 0.8652\n",
      "Epoch 9, Attribute attr_2, Validation Accuracy: 0.6933\n",
      "Epoch 9, Attribute attr_3, Validation Accuracy: 0.8470\n",
      "Epoch 9, Attribute attr_4, Validation Accuracy: 0.5625\n",
      "Epoch 9, Attribute attr_5, Validation Accuracy: 0.7195\n",
      "Epoch 9, Attribute attr_6, Validation Accuracy: 0.9593\n",
      "Epoch 9, Attribute attr_7, Validation Accuracy: 0.7402\n",
      "Epoch 9, Attribute attr_8, Validation Accuracy: 0.8685\n",
      "Epoch 9, Attribute attr_9, Validation Accuracy: 0.6592\n",
      "Epoch 9, Attribute attr_10, Validation Accuracy: 0.8343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attribute attr_1, Validation Accuracy: 0.8659\n",
      "Epoch 10, Attribute attr_2, Validation Accuracy: 0.6900\n",
      "Epoch 10, Attribute attr_3, Validation Accuracy: 0.8470\n",
      "Epoch 10, Attribute attr_4, Validation Accuracy: 0.5621\n",
      "Epoch 10, Attribute attr_5, Validation Accuracy: 0.7191\n",
      "Epoch 10, Attribute attr_6, Validation Accuracy: 0.9589\n",
      "Epoch 10, Attribute attr_7, Validation Accuracy: 0.7424\n",
      "Epoch 10, Attribute attr_8, Validation Accuracy: 0.8652\n",
      "Epoch 10, Attribute attr_9, Validation Accuracy: 0.6602\n",
      "Epoch 10, Attribute attr_10, Validation Accuracy: 0.8347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy for Attribute attr_1: 0.8641\n",
      "Final Test Accuracy for Attribute attr_2: 0.6959\n",
      "Final Test Accuracy for Attribute attr_3: 0.8398\n",
      "Final Test Accuracy for Attribute attr_4: 0.5549\n",
      "Final Test Accuracy for Attribute attr_5: 0.7009\n",
      "Final Test Accuracy for Attribute attr_6: 0.9611\n",
      "Final Test Accuracy for Attribute attr_7: 0.7504\n",
      "Final Test Accuracy for Attribute attr_8: 0.8634\n",
      "Final Test Accuracy for Attribute attr_9: 0.6486\n",
      "Final Test Accuracy for Attribute attr_10: 0.8307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Prepare and process data (assuming sarees_df is pre-loaded)\n",
    "sarees_df = sarees_df.drop(columns=['Category', 'len'])\n",
    "sarees_df = sarees_df.apply(\n",
    "    lambda col: col.fillna('dummy_value') if col.isna().all() \n",
    "    else col.fillna(col.mode()[0]) if col.dtype == 'object' or col.dtype.name == 'category' \n",
    "    else col\n",
    ")\n",
    "\n",
    "# Define attribute columns and category mappings\n",
    "attribute_columns_sarees = [f'attr_{i}' for i in range(1, 11)]\n",
    "category_mappings_sarees = {}\n",
    "\n",
    "for col in attribute_columns_sarees:\n",
    "    sarees_df[col] = pd.Categorical(sarees_df[col])\n",
    "    category_mappings_sarees[col] = dict(enumerate(sarees_df[col].cat.categories))\n",
    "    sarees_df[col] = sarees_df[col].cat.codes\n",
    "\n",
    "sarees_df['image_path'] = '/kaggle/input/visual-taxonomy/train_images/' + sarees_df['id'].apply(lambda x: f\"{x:06d}.jpg\")\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_df_sarees, temp_df = train_test_split(sarees_df, test_size=0.3, random_state=42)\n",
    "val_df_sarees, test_df_sarees = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define number of classes for each attribute\n",
    "num_classes_per_attribute = {\n",
    "    'attr_1': 4,  \n",
    "    'attr_2': 6,\n",
    "    'attr_3': 3,\n",
    "    'attr_4': 8,\n",
    "    'attr_5': 4,\n",
    "    'attr_6': 3,\n",
    "    'attr_7': 4,\n",
    "    'attr_8': 5,\n",
    "    'attr_9': 9,\n",
    "    'attr_10': 2\n",
    "}\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_per_attribute):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  \n",
    "        self.heads = nn.ModuleDict({\n",
    "            attr: nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "            for attr, num_classes in num_classes_per_attribute.items()\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.base_model(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_states = hidden_states[:, 0, :]\n",
    "        cls_hidden_states = self.dropout(cls_hidden_states)  \n",
    "        logits = {attr: self.heads[attr](cls_hidden_states) for attr in self.heads}\n",
    "        return logits\n",
    "\n",
    "base_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model_vit_sarees = MultiOutputModel(base_model, num_classes_per_attribute)\n",
    "\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "data_augmentation = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_str = str(self.df.iloc[idx][\"id\"]).zfill(6)\n",
    "        path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = {}\n",
    "        \n",
    "        for attr in attribute_columns_sarees:\n",
    "            labels[attr] = torch.tensor(self.df.iloc[idx][attr], dtype=torch.long)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset = CustomDataset(train_df_sarees, transform=data_augmentation)\n",
    "val_dataset = CustomDataset(val_df_sarees, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "test_dataset = CustomDataset(test_df_sarees, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)  # Increased batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vit_sarees.to(device)\n",
    "\n",
    "# Optimizer with Weight Decay\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_vit_sarees.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler: Cosine Annealing\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)  # Cosine annealing\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop \n",
    "\n",
    "num_epochs = 10  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_vit_sarees.train()\n",
    "    running_corrects = {attr: 0 for attr in attribute_columns_sarees}\n",
    "    total_samples = {attr: 0 for attr in attribute_columns_sarees}\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model_vit_sarees(pixel_values=images)\n",
    "            \n",
    "            losses = {}\n",
    "            \n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                loss = loss_fn(outputs[attr], target)\n",
    "                losses[attr] = loss\n",
    "\n",
    "                # Compute accuracy\n",
    "                \n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                running_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                total_samples[attr] += target.size(0)\n",
    "\n",
    "            total_loss = sum(losses.values())\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Validation Accuracy\n",
    "\n",
    "    model_vit_sarees.eval()\n",
    "    val_corrects = {attr: 0 for attr in attribute_columns_sarees}\n",
    "    val_samples = {attr: 0 for attr in attribute_columns_sarees}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model_vit_sarees(pixel_values=images)\n",
    "\n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "\n",
    "                loss = loss_fn(outputs[attr], target)\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                val_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                val_samples[attr] += target.size(0)\n",
    "\n",
    "    for attr in attribute_columns_sarees:\n",
    "        val_acc = val_corrects[attr] / val_samples[attr] if val_samples[attr] > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1}, Attribute {attr}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "model_vit_sarees.eval()\n",
    "test_corrects = {attr: 0 for attr in attribute_columns_sarees}\n",
    "test_samples = {attr: 0 for attr in attribute_columns_sarees}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        images = images.to(device)\n",
    "        outputs = model_vit_sarees(pixel_values=images)\n",
    "        \n",
    "        for attr in labels.keys():\n",
    "            target = labels[attr].long().to(device)\n",
    "            \n",
    "            # Compute test accuracy\n",
    "            _, preds = torch.max(outputs[attr], 1)\n",
    "            test_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "            test_samples[attr] += target.size(0)\n",
    "\n",
    "# Print final test accuracy for each attribute\n",
    "for attr in attribute_columns_sarees:\n",
    "    test_acc = test_corrects[attr] / test_samples[attr] if test_samples[attr] > 0 else 0\n",
    "    print(f\"Final Test Accuracy for Attribute {attr}: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T12:14:23.422589Z",
     "iopub.status.busy": "2024-11-10T12:14:23.422188Z",
     "iopub.status.idle": "2024-11-10T12:23:26.663974Z",
     "shell.execute_reply": "2024-11-10T12:23:26.662700Z",
     "shell.execute_reply.started": "2024-11-10T12:14:23.422548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/3123528654.py:121: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Training Epoch 1/10:   0%|          | 0/150 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_30/3123528654.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1/10:  99%|█████████▉| 149/150 [00:42<00:00,  3.67it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Attribute attr_1, Validation Accuracy: 0.6862\n",
      "Epoch 1, Attribute attr_2, Validation Accuracy: 0.8817\n",
      "Epoch 1, Attribute attr_3, Validation Accuracy: 0.8104\n",
      "Epoch 1, Attribute attr_4, Validation Accuracy: 0.9355\n",
      "Epoch 1, Attribute attr_5, Validation Accuracy: 0.9110\n",
      "Epoch 1, Attribute attr_6, Validation Accuracy: 0.7370\n",
      "Epoch 1, Attribute attr_7, Validation Accuracy: 0.7410\n",
      "Epoch 1, Attribute attr_8, Validation Accuracy: 0.9179\n",
      "Epoch 1, Attribute attr_9, Validation Accuracy: 0.9941\n",
      "Epoch 1, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Attribute attr_1, Validation Accuracy: 0.7546\n",
      "Epoch 2, Attribute attr_2, Validation Accuracy: 0.8768\n",
      "Epoch 2, Attribute attr_3, Validation Accuracy: 0.8113\n",
      "Epoch 2, Attribute attr_4, Validation Accuracy: 0.9472\n",
      "Epoch 2, Attribute attr_5, Validation Accuracy: 0.9130\n",
      "Epoch 2, Attribute attr_6, Validation Accuracy: 0.7664\n",
      "Epoch 2, Attribute attr_7, Validation Accuracy: 0.7419\n",
      "Epoch 2, Attribute attr_8, Validation Accuracy: 0.9374\n",
      "Epoch 2, Attribute attr_9, Validation Accuracy: 0.9971\n",
      "Epoch 2, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Attribute attr_1, Validation Accuracy: 0.7830\n",
      "Epoch 3, Attribute attr_2, Validation Accuracy: 0.8817\n",
      "Epoch 3, Attribute attr_3, Validation Accuracy: 0.8192\n",
      "Epoch 3, Attribute attr_4, Validation Accuracy: 0.9433\n",
      "Epoch 3, Attribute attr_5, Validation Accuracy: 0.9198\n",
      "Epoch 3, Attribute attr_6, Validation Accuracy: 0.7742\n",
      "Epoch 3, Attribute attr_7, Validation Accuracy: 0.7488\n",
      "Epoch 3, Attribute attr_8, Validation Accuracy: 0.9570\n",
      "Epoch 3, Attribute attr_9, Validation Accuracy: 0.9990\n",
      "Epoch 3, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Attribute attr_1, Validation Accuracy: 0.7898\n",
      "Epoch 4, Attribute attr_2, Validation Accuracy: 0.8895\n",
      "Epoch 4, Attribute attr_3, Validation Accuracy: 0.8240\n",
      "Epoch 4, Attribute attr_4, Validation Accuracy: 0.9345\n",
      "Epoch 4, Attribute attr_5, Validation Accuracy: 0.9247\n",
      "Epoch 4, Attribute attr_6, Validation Accuracy: 0.7781\n",
      "Epoch 4, Attribute attr_7, Validation Accuracy: 0.7556\n",
      "Epoch 4, Attribute attr_8, Validation Accuracy: 0.9492\n",
      "Epoch 4, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 4, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attribute attr_1, Validation Accuracy: 0.7996\n",
      "Epoch 5, Attribute attr_2, Validation Accuracy: 0.8886\n",
      "Epoch 5, Attribute attr_3, Validation Accuracy: 0.8182\n",
      "Epoch 5, Attribute attr_4, Validation Accuracy: 0.9433\n",
      "Epoch 5, Attribute attr_5, Validation Accuracy: 0.9218\n",
      "Epoch 5, Attribute attr_6, Validation Accuracy: 0.7771\n",
      "Epoch 5, Attribute attr_7, Validation Accuracy: 0.7546\n",
      "Epoch 5, Attribute attr_8, Validation Accuracy: 0.9589\n",
      "Epoch 5, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 5, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Attribute attr_1, Validation Accuracy: 0.8035\n",
      "Epoch 6, Attribute attr_2, Validation Accuracy: 0.8954\n",
      "Epoch 6, Attribute attr_3, Validation Accuracy: 0.8289\n",
      "Epoch 6, Attribute attr_4, Validation Accuracy: 0.9423\n",
      "Epoch 6, Attribute attr_5, Validation Accuracy: 0.9189\n",
      "Epoch 6, Attribute attr_6, Validation Accuracy: 0.7732\n",
      "Epoch 6, Attribute attr_7, Validation Accuracy: 0.7478\n",
      "Epoch 6, Attribute attr_8, Validation Accuracy: 0.9609\n",
      "Epoch 6, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 6, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Attribute attr_1, Validation Accuracy: 0.8133\n",
      "Epoch 7, Attribute attr_2, Validation Accuracy: 0.8954\n",
      "Epoch 7, Attribute attr_3, Validation Accuracy: 0.8260\n",
      "Epoch 7, Attribute attr_4, Validation Accuracy: 0.9433\n",
      "Epoch 7, Attribute attr_5, Validation Accuracy: 0.9228\n",
      "Epoch 7, Attribute attr_6, Validation Accuracy: 0.7859\n",
      "Epoch 7, Attribute attr_7, Validation Accuracy: 0.7654\n",
      "Epoch 7, Attribute attr_8, Validation Accuracy: 0.9629\n",
      "Epoch 7, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 7, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Attribute attr_1, Validation Accuracy: 0.8250\n",
      "Epoch 8, Attribute attr_2, Validation Accuracy: 0.8954\n",
      "Epoch 8, Attribute attr_3, Validation Accuracy: 0.8133\n",
      "Epoch 8, Attribute attr_4, Validation Accuracy: 0.9433\n",
      "Epoch 8, Attribute attr_5, Validation Accuracy: 0.9208\n",
      "Epoch 8, Attribute attr_6, Validation Accuracy: 0.7889\n",
      "Epoch 8, Attribute attr_7, Validation Accuracy: 0.7595\n",
      "Epoch 8, Attribute attr_8, Validation Accuracy: 0.9619\n",
      "Epoch 8, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 8, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Attribute attr_1, Validation Accuracy: 0.8289\n",
      "Epoch 9, Attribute attr_2, Validation Accuracy: 0.8954\n",
      "Epoch 9, Attribute attr_3, Validation Accuracy: 0.8250\n",
      "Epoch 9, Attribute attr_4, Validation Accuracy: 0.9453\n",
      "Epoch 9, Attribute attr_5, Validation Accuracy: 0.9228\n",
      "Epoch 9, Attribute attr_6, Validation Accuracy: 0.7791\n",
      "Epoch 9, Attribute attr_7, Validation Accuracy: 0.7722\n",
      "Epoch 9, Attribute attr_8, Validation Accuracy: 0.9629\n",
      "Epoch 9, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 9, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attribute attr_1, Validation Accuracy: 0.8211\n",
      "Epoch 10, Attribute attr_2, Validation Accuracy: 0.8954\n",
      "Epoch 10, Attribute attr_3, Validation Accuracy: 0.8250\n",
      "Epoch 10, Attribute attr_4, Validation Accuracy: 0.9433\n",
      "Epoch 10, Attribute attr_5, Validation Accuracy: 0.9228\n",
      "Epoch 10, Attribute attr_6, Validation Accuracy: 0.7849\n",
      "Epoch 10, Attribute attr_7, Validation Accuracy: 0.7615\n",
      "Epoch 10, Attribute attr_8, Validation Accuracy: 0.9638\n",
      "Epoch 10, Attribute attr_9, Validation Accuracy: 0.9980\n",
      "Epoch 10, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy for Attribute attr_1: 0.8076\n",
      "Final Test Accuracy for Attribute attr_2: 0.8857\n",
      "Final Test Accuracy for Attribute attr_3: 0.8320\n",
      "Final Test Accuracy for Attribute attr_4: 0.9346\n",
      "Final Test Accuracy for Attribute attr_5: 0.9316\n",
      "Final Test Accuracy for Attribute attr_6: 0.7695\n",
      "Final Test Accuracy for Attribute attr_7: 0.7656\n",
      "Final Test Accuracy for Attribute attr_8: 0.9648\n",
      "Final Test Accuracy for Attribute attr_9: 0.9941\n",
      "Final Test Accuracy for Attribute attr_10: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Prepare and process data \n",
    "kurtis_df = kurtis_df.drop(columns=['Category', 'len'])\n",
    "kurtis_df = kurtis_df.apply(\n",
    "    lambda col: col.fillna('dummy_value') if col.isna().all() \n",
    "    else col.fillna(col.mode()[0]) if col.dtype == 'object' or col.dtype.name == 'category' \n",
    "    else col\n",
    ")\n",
    "\n",
    "attribute_columns_kurtis = [f'attr_{i}' for i in range(1, 11)]\n",
    "category_mappings_kurtis = {}\n",
    "\n",
    "for col in attribute_columns_kurtis:\n",
    "    kurtis_df[col] = pd.Categorical(kurtis_df[col])\n",
    "    category_mappings_kurtis[col] = dict(enumerate(kurtis_df[col].cat.categories))\n",
    "    kurtis_df[col] = kurtis_df[col].cat.codes\n",
    "\n",
    "kurtis_df['image_path'] = '/kaggle/input/visual-taxonomy/train_images/' + kurtis_df['id'].apply(lambda x: f\"{x:06d}.jpg\")\n",
    "\n",
    "train_df_kurtis, temp_df_kurtis = train_test_split(kurtis_df, test_size=0.3, random_state=42)\n",
    "val_df_kurtis, test_df_kurtis = train_test_split(temp_df_kurtis, test_size=0.5, random_state=42)\n",
    "\n",
    "num_classes_per_attribute_kurtis = {\n",
    "    'attr_1': 13,\n",
    "    'attr_2': 2,\n",
    "    'attr_3': 2,\n",
    "    'attr_4': 2,\n",
    "    'attr_5': 2,\n",
    "    'attr_6': 2,\n",
    "    'attr_7': 2,\n",
    "    'attr_8': 3,\n",
    "    'attr_9': 2,\n",
    "    'attr_10': 1\n",
    "}\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_per_attribute):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  \n",
    "        self.heads = nn.ModuleDict({\n",
    "            attr: nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "            for attr, num_classes in num_classes_per_attribute.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.base_model(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_states = hidden_states[:, 0, :]\n",
    "        cls_hidden_states = self.dropout(cls_hidden_states) \n",
    "        logits = {attr: self.heads[attr](cls_hidden_states) for attr in self.heads}\n",
    "        return logits\n",
    "\n",
    "base_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model_vit_kurtis = MultiOutputModel(base_model, num_classes_per_attribute_kurtis)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_str = str(self.df.iloc[idx][\"id\"]).zfill(6)\n",
    "        path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = {}\n",
    "        for attr in attribute_columns_kurtis:\n",
    "            labels[attr] = torch.tensor(self.df.iloc[idx][attr], dtype=torch.long)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset_kurtis = CustomDataset(train_df_kurtis, transform=data_augmentation)\n",
    "val_dataset_kurtis = CustomDataset(val_df_kurtis, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "test_dataset_kurtis = CustomDataset(test_df_kurtis, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "\n",
    "train_loader_kurtis = DataLoader(train_dataset_kurtis, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader_kurtis = DataLoader(val_dataset_kurtis, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader_kurtis = DataLoader(test_dataset_kurtis, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vit_kurtis.to(device)\n",
    "\n",
    "# Optimizer with Weight Decay\n",
    "optimizer = torch.optim.AdamW(model_vit_kurtis.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning Rate Scheduler: Cosine Annealing\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop \n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_vit_kurtis.train()\n",
    "    running_corrects = {attr: 0 for attr in attribute_columns_kurtis}\n",
    "    total_samples = {attr: 0 for attr in attribute_columns_kurtis}\n",
    "\n",
    "    for images, labels in tqdm(train_loader_kurtis, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model_vit_kurtis(pixel_values=images)\n",
    "            losses = {}\n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                loss = loss_fn(outputs[attr], target)\n",
    "                losses[attr] = loss\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                running_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                total_samples[attr] += target.size(0)\n",
    "\n",
    "            total_loss = sum(losses.values())\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Accuracy\n",
    "    model_vit_kurtis.eval()\n",
    "    val_corrects = {attr: 0 for attr in attribute_columns_kurtis}\n",
    "    val_samples = {attr: 0 for attr in attribute_columns_kurtis}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_kurtis, desc=\"Validating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model_vit_kurtis(pixel_values=images)\n",
    "            for attr in labels.keys():\n",
    "                target = labels[attr].long().to(device)\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                val_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                val_samples[attr] += target.size(0)\n",
    "\n",
    "    for attr in attribute_columns_kurtis:\n",
    "        val_acc = val_corrects[attr] / val_samples[attr] if val_samples[attr] > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1}, Attribute {attr}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "model_vit_kurtis.eval()\n",
    "test_corrects = {attr: 0 for attr in attribute_columns_kurtis}\n",
    "test_samples = {attr: 0 for attr in attribute_columns_kurtis}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader_kurtis, desc=\"Testing\", leave=False):\n",
    "        images = images.to(device)\n",
    "        outputs = model_vit_kurtis(pixel_values=images)\n",
    "        \n",
    "        for attr in labels.keys():\n",
    "            target = labels[attr].long().to(device)\n",
    "            _, preds = torch.max(outputs[attr], 1)\n",
    "            test_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "            test_samples[attr] += target.size(0)\n",
    "\n",
    "for attr in attribute_columns_kurtis:\n",
    "    test_acc = test_corrects[attr] / test_samples[attr] if test_samples[attr] > 0 else 0\n",
    "    print(f\"Final Test Accuracy for Attribute {attr}: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T12:23:26.667215Z",
     "iopub.status.busy": "2024-11-10T12:23:26.666765Z",
     "iopub.status.idle": "2024-11-10T12:47:48.531580Z",
     "shell.execute_reply": "2024-11-10T12:47:48.530484Z",
     "shell.execute_reply.started": "2024-11-10T12:23:26.667164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/3633775563.py:119: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Training Epoch 1/10:   0%|          | 0/411 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_30/3633775563.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1/10: 100%|██████████| 411/411 [01:52<00:00,  4.07it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Attribute attr_1, Validation Accuracy: 0.7592\n",
      "Epoch 1, Attribute attr_2, Validation Accuracy: 0.8952\n",
      "Epoch 1, Attribute attr_3, Validation Accuracy: 0.7827\n",
      "Epoch 1, Attribute attr_4, Validation Accuracy: 0.9574\n",
      "Epoch 1, Attribute attr_5, Validation Accuracy: 0.6918\n",
      "Epoch 1, Attribute attr_6, Validation Accuracy: 0.9439\n",
      "Epoch 1, Attribute attr_7, Validation Accuracy: 0.9613\n",
      "Epoch 1, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 1, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 1, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Attribute attr_1, Validation Accuracy: 0.7816\n",
      "Epoch 2, Attribute attr_2, Validation Accuracy: 0.9023\n",
      "Epoch 2, Attribute attr_3, Validation Accuracy: 0.8075\n",
      "Epoch 2, Attribute attr_4, Validation Accuracy: 0.9634\n",
      "Epoch 2, Attribute attr_5, Validation Accuracy: 0.7152\n",
      "Epoch 2, Attribute attr_6, Validation Accuracy: 0.9442\n",
      "Epoch 2, Attribute attr_7, Validation Accuracy: 0.9688\n",
      "Epoch 2, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 2, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 2, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Attribute attr_1, Validation Accuracy: 0.7958\n",
      "Epoch 3, Attribute attr_2, Validation Accuracy: 0.9070\n",
      "Epoch 3, Attribute attr_3, Validation Accuracy: 0.8168\n",
      "Epoch 3, Attribute attr_4, Validation Accuracy: 0.9691\n",
      "Epoch 3, Attribute attr_5, Validation Accuracy: 0.7248\n",
      "Epoch 3, Attribute attr_6, Validation Accuracy: 0.9425\n",
      "Epoch 3, Attribute attr_7, Validation Accuracy: 0.9702\n",
      "Epoch 3, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 3, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 3, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Attribute attr_1, Validation Accuracy: 0.7997\n",
      "Epoch 4, Attribute attr_2, Validation Accuracy: 0.9098\n",
      "Epoch 4, Attribute attr_3, Validation Accuracy: 0.8178\n",
      "Epoch 4, Attribute attr_4, Validation Accuracy: 0.9698\n",
      "Epoch 4, Attribute attr_5, Validation Accuracy: 0.7326\n",
      "Epoch 4, Attribute attr_6, Validation Accuracy: 0.9471\n",
      "Epoch 4, Attribute attr_7, Validation Accuracy: 0.9698\n",
      "Epoch 4, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 4, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 4, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attribute attr_1, Validation Accuracy: 0.7976\n",
      "Epoch 5, Attribute attr_2, Validation Accuracy: 0.9141\n",
      "Epoch 5, Attribute attr_3, Validation Accuracy: 0.8143\n",
      "Epoch 5, Attribute attr_4, Validation Accuracy: 0.9716\n",
      "Epoch 5, Attribute attr_5, Validation Accuracy: 0.7376\n",
      "Epoch 5, Attribute attr_6, Validation Accuracy: 0.9489\n",
      "Epoch 5, Attribute attr_7, Validation Accuracy: 0.9755\n",
      "Epoch 5, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 5, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 5, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Attribute attr_1, Validation Accuracy: 0.8050\n",
      "Epoch 6, Attribute attr_2, Validation Accuracy: 0.9158\n",
      "Epoch 6, Attribute attr_3, Validation Accuracy: 0.8256\n",
      "Epoch 6, Attribute attr_4, Validation Accuracy: 0.9702\n",
      "Epoch 6, Attribute attr_5, Validation Accuracy: 0.7326\n",
      "Epoch 6, Attribute attr_6, Validation Accuracy: 0.9474\n",
      "Epoch 6, Attribute attr_7, Validation Accuracy: 0.9695\n",
      "Epoch 6, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 6, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 6, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Attribute attr_1, Validation Accuracy: 0.8008\n",
      "Epoch 7, Attribute attr_2, Validation Accuracy: 0.9134\n",
      "Epoch 7, Attribute attr_3, Validation Accuracy: 0.8189\n",
      "Epoch 7, Attribute attr_4, Validation Accuracy: 0.9698\n",
      "Epoch 7, Attribute attr_5, Validation Accuracy: 0.7454\n",
      "Epoch 7, Attribute attr_6, Validation Accuracy: 0.9446\n",
      "Epoch 7, Attribute attr_7, Validation Accuracy: 0.9673\n",
      "Epoch 7, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 7, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 7, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Attribute attr_1, Validation Accuracy: 0.8065\n",
      "Epoch 8, Attribute attr_2, Validation Accuracy: 0.9155\n",
      "Epoch 8, Attribute attr_3, Validation Accuracy: 0.8313\n",
      "Epoch 8, Attribute attr_4, Validation Accuracy: 0.9673\n",
      "Epoch 8, Attribute attr_5, Validation Accuracy: 0.7411\n",
      "Epoch 8, Attribute attr_6, Validation Accuracy: 0.9467\n",
      "Epoch 8, Attribute attr_7, Validation Accuracy: 0.9648\n",
      "Epoch 8, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 8, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 8, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Attribute attr_1, Validation Accuracy: 0.8061\n",
      "Epoch 9, Attribute attr_2, Validation Accuracy: 0.9165\n",
      "Epoch 9, Attribute attr_3, Validation Accuracy: 0.8292\n",
      "Epoch 9, Attribute attr_4, Validation Accuracy: 0.9680\n",
      "Epoch 9, Attribute attr_5, Validation Accuracy: 0.7422\n",
      "Epoch 9, Attribute attr_6, Validation Accuracy: 0.9467\n",
      "Epoch 9, Attribute attr_7, Validation Accuracy: 0.9663\n",
      "Epoch 9, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 9, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 9, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attribute attr_1, Validation Accuracy: 0.8047\n",
      "Epoch 10, Attribute attr_2, Validation Accuracy: 0.9187\n",
      "Epoch 10, Attribute attr_3, Validation Accuracy: 0.8281\n",
      "Epoch 10, Attribute attr_4, Validation Accuracy: 0.9680\n",
      "Epoch 10, Attribute attr_5, Validation Accuracy: 0.7436\n",
      "Epoch 10, Attribute attr_6, Validation Accuracy: 0.9467\n",
      "Epoch 10, Attribute attr_7, Validation Accuracy: 0.9663\n",
      "Epoch 10, Attribute attr_8, Validation Accuracy: 0.9986\n",
      "Epoch 10, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 10, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy for Attribute attr_1: 0.7998\n",
      "Final Test Accuracy for Attribute attr_2: 0.9038\n",
      "Final Test Accuracy for Attribute attr_3: 0.8250\n",
      "Final Test Accuracy for Attribute attr_4: 0.9681\n",
      "Final Test Accuracy for Attribute attr_5: 0.7316\n",
      "Final Test Accuracy for Attribute attr_6: 0.9457\n",
      "Final Test Accuracy for Attribute attr_7: 0.9673\n",
      "Final Test Accuracy for Attribute attr_8: 0.9996\n",
      "Final Test Accuracy for Attribute attr_9: 1.0000\n",
      "Final Test Accuracy for Attribute attr_10: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "women_tshirts_df = women_tshirts_df.drop(columns=['Category', 'len'])\n",
    "women_tshirts_df = women_tshirts_df.apply(\n",
    "    lambda col: col.fillna('dummy_value') if col.isna().all() \n",
    "    else col.fillna(col.mode()[0]) if col.dtype == 'object' or col.dtype.name == 'category' \n",
    "    else col\n",
    ")\n",
    "\n",
    "attribute_columns_women_tshirts = [f'attr_{i}' for i in range(1, 11)]\n",
    "category_mappings_women_tshirts = {}\n",
    "\n",
    "for col in attribute_columns_women_tshirts:\n",
    "    women_tshirts_df[col] = pd.Categorical(women_tshirts_df[col])\n",
    "    category_mappings_women_tshirts[col] = dict(enumerate(women_tshirts_df[col].cat.categories))\n",
    "    women_tshirts_df[col] = women_tshirts_df[col].cat.codes\n",
    "\n",
    "women_tshirts_df['image_path'] = '/kaggle/input/visual-taxonomy/train_images/' + women_tshirts_df['id'].apply(lambda x: f\"{x:06d}.jpg\")\n",
    "\n",
    "train_df_women_tshirts, temp_df = train_test_split(women_tshirts_df, test_size=0.3, random_state=42)\n",
    "val_df_women_tshirts, test_df_women_tshirts = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "num_classes_per_attribute_women_tshirts = {\n",
    "    'attr_1': 7,\n",
    "    'attr_2': 3,\n",
    "    'attr_3': 3,\n",
    "    'attr_4': 3,\n",
    "    'attr_5': 6,\n",
    "    'attr_6': 3,\n",
    "    'attr_7': 2,\n",
    "    'attr_8': 2,\n",
    "    'attr_9': 1,\n",
    "    'attr_10': 1\n",
    "}\n",
    "\n",
    "# Model with Dropout and Learning Rate Scheduling for women_tshirts\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_per_attribute):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  # Adding Dropout\n",
    "        self.heads = nn.ModuleDict({\n",
    "            attr: nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "            for attr, num_classes in num_classes_per_attribute.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.base_model(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_states = hidden_states[:, 0, :]\n",
    "        cls_hidden_states = self.dropout(cls_hidden_states)  # Apply dropout\n",
    "        logits = {attr: self.heads[attr](cls_hidden_states) for attr in self.heads}\n",
    "        return logits\n",
    "\n",
    "base_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model_vit_women_tshirts = MultiOutputModel(base_model, num_classes_per_attribute_women_tshirts)\n",
    "\n",
    "# Data Augmentation for women_tshirts\n",
    "data_augmentation = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_str = str(self.df.iloc[idx][\"id\"]).zfill(6)\n",
    "        path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = {}\n",
    "        \n",
    "        for attr in attribute_columns_women_tshirts:\n",
    "            labels[attr] = torch.tensor(self.df.iloc[idx][attr], dtype=torch.long)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset = CustomDataset(train_df_women_tshirts, transform=data_augmentation)\n",
    "val_dataset = CustomDataset(val_df_women_tshirts, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "test_dataset = CustomDataset(test_df_women_tshirts, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vit_women_tshirts.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate Scheduler for women_tshirts\n",
    "optimizer = torch.optim.AdamW(model_vit_women_tshirts.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop for women_tshirts\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_vit_women_tshirts.train()\n",
    "    running_corrects = {attr: 0 for attr in attribute_columns_women_tshirts}\n",
    "    total_samples = {attr: 0 for attr in attribute_columns_women_tshirts}\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model_vit_women_tshirts(pixel_values=images)\n",
    "            \n",
    "            losses = {}\n",
    "            \n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                loss = loss_fn(outputs[attr], target)\n",
    "                losses[attr] = loss\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                running_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                total_samples[attr] += target.size(0)\n",
    "\n",
    "            total_loss = sum(losses.values())\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Accuracy\n",
    "    model_vit_women_tshirts.eval()\n",
    "    val_corrects = {attr: 0 for attr in attribute_columns_women_tshirts}\n",
    "    val_samples = {attr: 0 for attr in attribute_columns_women_tshirts}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model_vit_women_tshirts(pixel_values=images)\n",
    "\n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                val_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                val_samples[attr] += target.size(0)\n",
    "\n",
    "    for attr in attribute_columns_women_tshirts:\n",
    "        val_acc = val_corrects[attr] / val_samples[attr] if val_samples[attr] > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1}, Attribute {attr}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Final Test Accuracy\n",
    "model_vit_women_tshirts.eval()\n",
    "test_corrects = {attr: 0 for attr in attribute_columns_women_tshirts}\n",
    "test_samples = {attr: 0 for attr in attribute_columns_women_tshirts}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        images = images.to(device)\n",
    "        outputs = model_vit_women_tshirts(pixel_values=images)\n",
    "        \n",
    "        for attr in labels.keys():\n",
    "            target = labels[attr].long().to(device)\n",
    "            _, preds = torch.max(outputs[attr], 1)\n",
    "            test_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "            test_samples[attr] += target.size(0)\n",
    "\n",
    "# Print final test accuracy for each attribute\n",
    "for attr in attribute_columns_women_tshirts:\n",
    "    test_acc = test_corrects[attr] / test_samples[attr] if test_samples[attr] > 0 else 0\n",
    "    print(f\"Final Test Accuracy for Attribute {attr}: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T12:47:48.534830Z",
     "iopub.status.busy": "2024-11-10T12:47:48.534138Z",
     "iopub.status.idle": "2024-11-10T13:12:27.434782Z",
     "shell.execute_reply": "2024-11-10T13:12:27.433618Z",
     "shell.execute_reply.started": "2024-11-10T12:47:48.534777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/2785479414.py:119: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Training Epoch 1/10:   0%|          | 0/416 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_30/2785479414.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1/10: 100%|██████████| 416/416 [01:54<00:00,  4.00it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Attribute attr_1, Validation Accuracy: 0.5851\n",
      "Epoch 1, Attribute attr_2, Validation Accuracy: 0.7425\n",
      "Epoch 1, Attribute attr_3, Validation Accuracy: 0.8446\n",
      "Epoch 1, Attribute attr_4, Validation Accuracy: 0.6875\n",
      "Epoch 1, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 1, Attribute attr_6, Validation Accuracy: 0.8923\n",
      "Epoch 1, Attribute attr_7, Validation Accuracy: 0.8334\n",
      "Epoch 1, Attribute attr_8, Validation Accuracy: 0.8127\n",
      "Epoch 1, Attribute attr_9, Validation Accuracy: 0.8225\n",
      "Epoch 1, Attribute attr_10, Validation Accuracy: 0.8015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Attribute attr_1, Validation Accuracy: 0.6226\n",
      "Epoch 2, Attribute attr_2, Validation Accuracy: 0.7538\n",
      "Epoch 2, Attribute attr_3, Validation Accuracy: 0.8548\n",
      "Epoch 2, Attribute attr_4, Validation Accuracy: 0.7078\n",
      "Epoch 2, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 2, Attribute attr_6, Validation Accuracy: 0.8990\n",
      "Epoch 2, Attribute attr_7, Validation Accuracy: 0.8320\n",
      "Epoch 2, Attribute attr_8, Validation Accuracy: 0.8281\n",
      "Epoch 2, Attribute attr_9, Validation Accuracy: 0.8348\n",
      "Epoch 2, Attribute attr_10, Validation Accuracy: 0.8162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Attribute attr_1, Validation Accuracy: 0.6342\n",
      "Epoch 3, Attribute attr_2, Validation Accuracy: 0.7576\n",
      "Epoch 3, Attribute attr_3, Validation Accuracy: 0.8523\n",
      "Epoch 3, Attribute attr_4, Validation Accuracy: 0.7155\n",
      "Epoch 3, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 3, Attribute attr_6, Validation Accuracy: 0.8923\n",
      "Epoch 3, Attribute attr_7, Validation Accuracy: 0.8394\n",
      "Epoch 3, Attribute attr_8, Validation Accuracy: 0.8225\n",
      "Epoch 3, Attribute attr_9, Validation Accuracy: 0.8355\n",
      "Epoch 3, Attribute attr_10, Validation Accuracy: 0.8197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Attribute attr_1, Validation Accuracy: 0.6563\n",
      "Epoch 4, Attribute attr_2, Validation Accuracy: 0.7618\n",
      "Epoch 4, Attribute attr_3, Validation Accuracy: 0.8583\n",
      "Epoch 4, Attribute attr_4, Validation Accuracy: 0.7226\n",
      "Epoch 4, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 4, Attribute attr_6, Validation Accuracy: 0.8997\n",
      "Epoch 4, Attribute attr_7, Validation Accuracy: 0.8341\n",
      "Epoch 4, Attribute attr_8, Validation Accuracy: 0.8236\n",
      "Epoch 4, Attribute attr_9, Validation Accuracy: 0.8425\n",
      "Epoch 4, Attribute attr_10, Validation Accuracy: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attribute attr_1, Validation Accuracy: 0.6657\n",
      "Epoch 5, Attribute attr_2, Validation Accuracy: 0.7685\n",
      "Epoch 5, Attribute attr_3, Validation Accuracy: 0.8604\n",
      "Epoch 5, Attribute attr_4, Validation Accuracy: 0.7306\n",
      "Epoch 5, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 5, Attribute attr_6, Validation Accuracy: 0.9004\n",
      "Epoch 5, Attribute attr_7, Validation Accuracy: 0.8474\n",
      "Epoch 5, Attribute attr_8, Validation Accuracy: 0.8390\n",
      "Epoch 5, Attribute attr_9, Validation Accuracy: 0.8474\n",
      "Epoch 5, Attribute attr_10, Validation Accuracy: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Attribute attr_1, Validation Accuracy: 0.6717\n",
      "Epoch 6, Attribute attr_2, Validation Accuracy: 0.7643\n",
      "Epoch 6, Attribute attr_3, Validation Accuracy: 0.8632\n",
      "Epoch 6, Attribute attr_4, Validation Accuracy: 0.7285\n",
      "Epoch 6, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 6, Attribute attr_6, Validation Accuracy: 0.9004\n",
      "Epoch 6, Attribute attr_7, Validation Accuracy: 0.8401\n",
      "Epoch 6, Attribute attr_8, Validation Accuracy: 0.8295\n",
      "Epoch 6, Attribute attr_9, Validation Accuracy: 0.8555\n",
      "Epoch 6, Attribute attr_10, Validation Accuracy: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Attribute attr_1, Validation Accuracy: 0.6780\n",
      "Epoch 7, Attribute attr_2, Validation Accuracy: 0.7675\n",
      "Epoch 7, Attribute attr_3, Validation Accuracy: 0.8639\n",
      "Epoch 7, Attribute attr_4, Validation Accuracy: 0.7327\n",
      "Epoch 7, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 7, Attribute attr_6, Validation Accuracy: 0.8986\n",
      "Epoch 7, Attribute attr_7, Validation Accuracy: 0.8450\n",
      "Epoch 7, Attribute attr_8, Validation Accuracy: 0.8337\n",
      "Epoch 7, Attribute attr_9, Validation Accuracy: 0.8544\n",
      "Epoch 7, Attribute attr_10, Validation Accuracy: 0.8243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Attribute attr_1, Validation Accuracy: 0.6812\n",
      "Epoch 8, Attribute attr_2, Validation Accuracy: 0.7675\n",
      "Epoch 8, Attribute attr_3, Validation Accuracy: 0.8646\n",
      "Epoch 8, Attribute attr_4, Validation Accuracy: 0.7348\n",
      "Epoch 8, Attribute attr_5, Validation Accuracy: 0.9902\n",
      "Epoch 8, Attribute attr_6, Validation Accuracy: 0.8990\n",
      "Epoch 8, Attribute attr_7, Validation Accuracy: 0.8464\n",
      "Epoch 8, Attribute attr_8, Validation Accuracy: 0.8337\n",
      "Epoch 8, Attribute attr_9, Validation Accuracy: 0.8513\n",
      "Epoch 8, Attribute attr_10, Validation Accuracy: 0.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Attribute attr_1, Validation Accuracy: 0.6812\n",
      "Epoch 9, Attribute attr_2, Validation Accuracy: 0.7710\n",
      "Epoch 9, Attribute attr_3, Validation Accuracy: 0.8643\n",
      "Epoch 9, Attribute attr_4, Validation Accuracy: 0.7376\n",
      "Epoch 9, Attribute attr_5, Validation Accuracy: 0.9905\n",
      "Epoch 9, Attribute attr_6, Validation Accuracy: 0.8990\n",
      "Epoch 9, Attribute attr_7, Validation Accuracy: 0.8478\n",
      "Epoch 9, Attribute attr_8, Validation Accuracy: 0.8351\n",
      "Epoch 9, Attribute attr_9, Validation Accuracy: 0.8513\n",
      "Epoch 9, Attribute attr_10, Validation Accuracy: 0.8278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attribute attr_1, Validation Accuracy: 0.6808\n",
      "Epoch 10, Attribute attr_2, Validation Accuracy: 0.7699\n",
      "Epoch 10, Attribute attr_3, Validation Accuracy: 0.8636\n",
      "Epoch 10, Attribute attr_4, Validation Accuracy: 0.7369\n",
      "Epoch 10, Attribute attr_5, Validation Accuracy: 0.9905\n",
      "Epoch 10, Attribute attr_6, Validation Accuracy: 0.9011\n",
      "Epoch 10, Attribute attr_7, Validation Accuracy: 0.8488\n",
      "Epoch 10, Attribute attr_8, Validation Accuracy: 0.8365\n",
      "Epoch 10, Attribute attr_9, Validation Accuracy: 0.8499\n",
      "Epoch 10, Attribute attr_10, Validation Accuracy: 0.8271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy for Attribute attr_1: 0.6734\n",
      "Final Test Accuracy for Attribute attr_2: 0.7724\n",
      "Final Test Accuracy for Attribute attr_3: 0.8716\n",
      "Final Test Accuracy for Attribute attr_4: 0.7461\n",
      "Final Test Accuracy for Attribute attr_5: 0.9895\n",
      "Final Test Accuracy for Attribute attr_6: 0.9021\n",
      "Final Test Accuracy for Attribute attr_7: 0.8558\n",
      "Final Test Accuracy for Attribute attr_8: 0.8478\n",
      "Final Test Accuracy for Attribute attr_9: 0.8365\n",
      "Final Test Accuracy for Attribute attr_10: 0.8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "women_tops_df = women_tops_df.drop(columns=['Category', 'len'])\n",
    "women_tops_df = women_tops_df.apply(\n",
    "    lambda col: col.fillna('dummy_value') if col.isna().all() \n",
    "    else col.fillna(col.mode()[0]) if col.dtype == 'object' or col.dtype.name == 'category' \n",
    "    else col\n",
    ")\n",
    "\n",
    "attribute_columns_women_tops = [f'attr_{i}' for i in range(1, 11)]\n",
    "category_mappings_women_tops = {}\n",
    "\n",
    "for col in attribute_columns_women_tops:\n",
    "    women_tops_df[col] = pd.Categorical(women_tops_df[col])\n",
    "    category_mappings_women_tops[col] = dict(enumerate(women_tops_df[col].cat.categories))\n",
    "    women_tops_df[col] = women_tops_df[col].cat.codes\n",
    "\n",
    "women_tops_df['image_path'] = '/kaggle/input/visual-taxonomy/train_images/' + women_tops_df['id'].apply(lambda x: f\"{x:06d}.jpg\")\n",
    "\n",
    "train_df_women_tops, temp_df = train_test_split(women_tops_df, test_size=0.3, random_state=42)\n",
    "val_df_women_tops, test_df_women_tops = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "num_classes_per_attribute_women_tops = {\n",
    "    'attr_1': 12,\n",
    "    'attr_2': 4,\n",
    "    'attr_3': 2,\n",
    "    'attr_4': 7,\n",
    "    'attr_5': 2,\n",
    "    'attr_6': 3,\n",
    "    'attr_7': 6,\n",
    "    'attr_8': 4,\n",
    "    'attr_9': 4,\n",
    "    'attr_10': 6\n",
    "}\n",
    "\n",
    "# Model with Dropout and Learning Rate Scheduling for women_tops\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_per_attribute):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  # Adding Dropout\n",
    "        self.heads = nn.ModuleDict({\n",
    "            attr: nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "            for attr, num_classes in num_classes_per_attribute.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.base_model(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_states = hidden_states[:, 0, :]\n",
    "        cls_hidden_states = self.dropout(cls_hidden_states)  # Apply dropout\n",
    "        logits = {attr: self.heads[attr](cls_hidden_states) for attr in self.heads}\n",
    "        return logits\n",
    "\n",
    "base_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model_vit_women_tops = MultiOutputModel(base_model, num_classes_per_attribute_women_tops)\n",
    "\n",
    "# Data Augmentation for women_tops\n",
    "data_augmentation = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_str = str(self.df.iloc[idx][\"id\"]).zfill(6)\n",
    "        path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = {}\n",
    "        \n",
    "        for attr in attribute_columns_women_tops:\n",
    "            labels[attr] = torch.tensor(self.df.iloc[idx][attr], dtype=torch.long)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset = CustomDataset(train_df_women_tops, transform=data_augmentation)\n",
    "val_dataset = CustomDataset(val_df_women_tops, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "test_dataset = CustomDataset(test_df_women_tops, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vit_women_tops.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate Scheduler for women_tops\n",
    "optimizer = torch.optim.AdamW(model_vit_women_tops.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop for women_tops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_vit_women_tops.train()\n",
    "    running_corrects = {attr: 0 for attr in attribute_columns_women_tops}\n",
    "    total_samples = {attr: 0 for attr in attribute_columns_women_tops}\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model_vit_women_tops(pixel_values=images)\n",
    "            \n",
    "            losses = {}\n",
    "            \n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                loss = loss_fn(outputs[attr], target)\n",
    "                losses[attr] = loss\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                running_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                total_samples[attr] += target.size(0)\n",
    "\n",
    "            total_loss = sum(losses.values())\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Accuracy\n",
    "    model_vit_women_tops.eval()\n",
    "    val_corrects = {attr: 0 for attr in attribute_columns_women_tops}\n",
    "    val_samples = {attr: 0 for attr in attribute_columns_women_tops}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model_vit_women_tops(pixel_values=images)\n",
    "\n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                val_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                val_samples[attr] += target.size(0)\n",
    "\n",
    "    for attr in attribute_columns_women_tops:\n",
    "        val_acc = val_corrects[attr] / val_samples[attr] if val_samples[attr] > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1}, Attribute {attr}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Final Test Accuracy\n",
    "model_vit_women_tops.eval()\n",
    "test_corrects = {attr: 0 for attr in attribute_columns_women_tops}\n",
    "test_samples = {attr: 0 for attr in attribute_columns_women_tops}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        images = images.to(device)\n",
    "        outputs = model_vit_women_tops(pixel_values=images)\n",
    "        \n",
    "        for attr in labels.keys():\n",
    "            target = labels[attr].long().to(device)\n",
    "            _, preds = torch.max(outputs[attr], 1)\n",
    "            test_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "            test_samples[attr] += target.size(0)\n",
    "\n",
    "# Print final test accuracy for each attribute\n",
    "for attr in attribute_columns_women_tops:\n",
    "    test_acc = test_corrects[attr] / test_samples[attr] if test_samples[attr] > 0 else 0\n",
    "    print(f\"Final Test Accuracy for Attribute {attr}: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:15:06.588136Z",
     "iopub.status.busy": "2024-11-10T13:15:06.587752Z",
     "iopub.status.idle": "2024-11-10T13:24:40.779355Z",
     "shell.execute_reply": "2024-11-10T13:24:40.778388Z",
     "shell.execute_reply.started": "2024-11-10T13:15:06.588098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/567498933.py:119: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Training Epoch 1/10:   0%|          | 0/159 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_30/567498933.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1/10: 100%|██████████| 159/159 [00:44<00:00,  3.66it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Attribute attr_1, Validation Accuracy: 0.6165\n",
      "Epoch 1, Attribute attr_2, Validation Accuracy: 0.8798\n",
      "Epoch 1, Attribute attr_3, Validation Accuracy: 0.8376\n",
      "Epoch 1, Attribute attr_4, Validation Accuracy: 0.7596\n",
      "Epoch 1, Attribute attr_5, Validation Accuracy: 0.9633\n",
      "Epoch 1, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 1, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 1, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 1, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 1, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Attribute attr_1, Validation Accuracy: 0.6183\n",
      "Epoch 2, Attribute attr_2, Validation Accuracy: 0.8642\n",
      "Epoch 2, Attribute attr_3, Validation Accuracy: 0.8257\n",
      "Epoch 2, Attribute attr_4, Validation Accuracy: 0.7119\n",
      "Epoch 2, Attribute attr_5, Validation Accuracy: 0.9771\n",
      "Epoch 2, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 2, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 2, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 2, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 2, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Attribute attr_1, Validation Accuracy: 0.6284\n",
      "Epoch 3, Attribute attr_2, Validation Accuracy: 0.8826\n",
      "Epoch 3, Attribute attr_3, Validation Accuracy: 0.8468\n",
      "Epoch 3, Attribute attr_4, Validation Accuracy: 0.7734\n",
      "Epoch 3, Attribute attr_5, Validation Accuracy: 0.9752\n",
      "Epoch 3, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 3, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 3, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 3, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 3, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Attribute attr_1, Validation Accuracy: 0.6431\n",
      "Epoch 4, Attribute attr_2, Validation Accuracy: 0.8835\n",
      "Epoch 4, Attribute attr_3, Validation Accuracy: 0.8450\n",
      "Epoch 4, Attribute attr_4, Validation Accuracy: 0.7881\n",
      "Epoch 4, Attribute attr_5, Validation Accuracy: 0.9725\n",
      "Epoch 4, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 4, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 4, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 4, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 4, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attribute attr_1, Validation Accuracy: 0.6294\n",
      "Epoch 5, Attribute attr_2, Validation Accuracy: 0.8853\n",
      "Epoch 5, Attribute attr_3, Validation Accuracy: 0.8349\n",
      "Epoch 5, Attribute attr_4, Validation Accuracy: 0.7826\n",
      "Epoch 5, Attribute attr_5, Validation Accuracy: 0.9716\n",
      "Epoch 5, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 5, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 5, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 5, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 5, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Attribute attr_1, Validation Accuracy: 0.6560\n",
      "Epoch 6, Attribute attr_2, Validation Accuracy: 0.8789\n",
      "Epoch 6, Attribute attr_3, Validation Accuracy: 0.8330\n",
      "Epoch 6, Attribute attr_4, Validation Accuracy: 0.7954\n",
      "Epoch 6, Attribute attr_5, Validation Accuracy: 0.9706\n",
      "Epoch 6, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 6, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 6, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 6, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 6, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Attribute attr_1, Validation Accuracy: 0.6404\n",
      "Epoch 7, Attribute attr_2, Validation Accuracy: 0.8716\n",
      "Epoch 7, Attribute attr_3, Validation Accuracy: 0.8349\n",
      "Epoch 7, Attribute attr_4, Validation Accuracy: 0.7789\n",
      "Epoch 7, Attribute attr_5, Validation Accuracy: 0.9716\n",
      "Epoch 7, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 7, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 7, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 7, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 7, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Attribute attr_1, Validation Accuracy: 0.6468\n",
      "Epoch 8, Attribute attr_2, Validation Accuracy: 0.8697\n",
      "Epoch 8, Attribute attr_3, Validation Accuracy: 0.8367\n",
      "Epoch 8, Attribute attr_4, Validation Accuracy: 0.7972\n",
      "Epoch 8, Attribute attr_5, Validation Accuracy: 0.9716\n",
      "Epoch 8, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 8, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 8, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 8, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 8, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Attribute attr_1, Validation Accuracy: 0.6541\n",
      "Epoch 9, Attribute attr_2, Validation Accuracy: 0.8734\n",
      "Epoch 9, Attribute attr_3, Validation Accuracy: 0.8294\n",
      "Epoch 9, Attribute attr_4, Validation Accuracy: 0.7862\n",
      "Epoch 9, Attribute attr_5, Validation Accuracy: 0.9706\n",
      "Epoch 9, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 9, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 9, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 9, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 9, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attribute attr_1, Validation Accuracy: 0.6505\n",
      "Epoch 10, Attribute attr_2, Validation Accuracy: 0.8716\n",
      "Epoch 10, Attribute attr_3, Validation Accuracy: 0.8303\n",
      "Epoch 10, Attribute attr_4, Validation Accuracy: 0.7908\n",
      "Epoch 10, Attribute attr_5, Validation Accuracy: 0.9725\n",
      "Epoch 10, Attribute attr_6, Validation Accuracy: 1.0000\n",
      "Epoch 10, Attribute attr_7, Validation Accuracy: 1.0000\n",
      "Epoch 10, Attribute attr_8, Validation Accuracy: 1.0000\n",
      "Epoch 10, Attribute attr_9, Validation Accuracy: 1.0000\n",
      "Epoch 10, Attribute attr_10, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy for Attribute attr_1: 0.6590\n",
      "Final Test Accuracy for Attribute attr_2: 0.8698\n",
      "Final Test Accuracy for Attribute attr_3: 0.8405\n",
      "Final Test Accuracy for Attribute attr_4: 0.7984\n",
      "Final Test Accuracy for Attribute attr_5: 0.9578\n",
      "Final Test Accuracy for Attribute attr_6: 1.0000\n",
      "Final Test Accuracy for Attribute attr_7: 1.0000\n",
      "Final Test Accuracy for Attribute attr_8: 1.0000\n",
      "Final Test Accuracy for Attribute attr_9: 1.0000\n",
      "Final Test Accuracy for Attribute attr_10: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "men_tshirts_df = men_tshirts_df.drop(columns=['Category', 'len'])\n",
    "men_tshirts_df = men_tshirts_df.apply(\n",
    "    lambda col: col.fillna('dummy_value') if col.isna().all() \n",
    "    else col.fillna(col.mode()[0]) if col.dtype == 'object' or col.dtype.name == 'category' \n",
    "    else col\n",
    ")\n",
    "\n",
    "attribute_columns_men_tshirts = [f'attr_{i}' for i in range(1, 11)]\n",
    "category_mappings_men_tshirts = {}\n",
    "\n",
    "for col in attribute_columns_men_tshirts:\n",
    "    men_tshirts_df[col] = pd.Categorical(men_tshirts_df[col])\n",
    "    category_mappings_men_tshirts[col] = dict(enumerate(men_tshirts_df[col].cat.categories))\n",
    "    men_tshirts_df[col] = men_tshirts_df[col].cat.codes\n",
    "\n",
    "men_tshirts_df['image_path'] = '/kaggle/input/visual-taxonomy/train_images/' + men_tshirts_df['id'].apply(lambda x: f\"{x:06d}.jpg\")\n",
    "\n",
    "train_df_men_tshirts, temp_df = train_test_split(men_tshirts_df, test_size=0.3, random_state=42)\n",
    "val_df_men_tshirts, test_df_men_tshirts = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "num_classes_per_attribute_men_tshirts = {\n",
    "    'attr_1': 4,\n",
    "    'attr_2': 2,\n",
    "    'attr_3': 2,\n",
    "    'attr_4': 3,\n",
    "    'attr_5': 3,\n",
    "    'attr_6': 1,\n",
    "    'attr_7': 1,\n",
    "    'attr_8': 1,\n",
    "    'attr_9': 1,\n",
    "    'attr_10': 1\n",
    "}\n",
    "\n",
    "# Model with Dropout and Learning Rate Scheduling for men_tshirts\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_per_attribute):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  # Adding Dropout\n",
    "        self.heads = nn.ModuleDict({\n",
    "            attr: nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "            for attr, num_classes in num_classes_per_attribute.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.base_model(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_states = hidden_states[:, 0, :]\n",
    "        cls_hidden_states = self.dropout(cls_hidden_states)  # Apply dropout\n",
    "        logits = {attr: self.heads[attr](cls_hidden_states) for attr in self.heads}\n",
    "        return logits\n",
    "\n",
    "base_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model_vit_men_tshirts = MultiOutputModel(base_model, num_classes_per_attribute_men_tshirts)\n",
    "\n",
    "# Data Augmentation for men_tshirts\n",
    "data_augmentation = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_str = str(self.df.iloc[idx][\"id\"]).zfill(6)\n",
    "        path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = {}\n",
    "        \n",
    "        for attr in attribute_columns_men_tshirts:\n",
    "            labels[attr] = torch.tensor(self.df.iloc[idx][attr], dtype=torch.long)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset = CustomDataset(train_df_men_tshirts, transform=data_augmentation)\n",
    "val_dataset = CustomDataset(val_df_men_tshirts, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "test_dataset = CustomDataset(test_df_men_tshirts, transform=T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vit_men_tshirts.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate Scheduler for men_tshirts\n",
    "optimizer = torch.optim.AdamW(model_vit_men_tshirts.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop for men_tshirts\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_vit_men_tshirts.train()\n",
    "    running_corrects = {attr: 0 for attr in attribute_columns_men_tshirts}\n",
    "    total_samples = {attr: 0 for attr in attribute_columns_men_tshirts}\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model_vit_men_tshirts(pixel_values=images)\n",
    "            \n",
    "            losses = {}\n",
    "            \n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                loss = loss_fn(outputs[attr], target)\n",
    "                losses[attr] = loss\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                running_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                total_samples[attr] += target.size(0)\n",
    "\n",
    "            total_loss = sum(losses.values())\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Accuracy\n",
    "    model_vit_men_tshirts.eval()\n",
    "    val_corrects = {attr: 0 for attr in attribute_columns_men_tshirts}\n",
    "    val_samples = {attr: 0 for attr in attribute_columns_men_tshirts}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model_vit_men_tshirts(pixel_values=images)\n",
    "\n",
    "            for attr in labels.keys():\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                target = labels[attr].long().to(device)\n",
    "                _, preds = torch.max(outputs[attr], 1)\n",
    "                val_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "                val_samples[attr] += target.size(0)\n",
    "\n",
    "    for attr in attribute_columns_men_tshirts:\n",
    "        val_acc = val_corrects[attr] / val_samples[attr] if val_samples[attr] > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1}, Attribute {attr}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Final Test Accuracy\n",
    "model_vit_men_tshirts.eval()\n",
    "test_corrects = {attr: 0 for attr in attribute_columns_men_tshirts}\n",
    "test_samples = {attr: 0 for attr in attribute_columns_men_tshirts}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        images = images.to(device)\n",
    "        outputs = model_vit_men_tshirts(pixel_values=images)\n",
    "        \n",
    "        for attr in labels.keys():\n",
    "            target = labels[attr].long().to(device)\n",
    "            _, preds = torch.max(outputs[attr], 1)\n",
    "            test_corrects[attr] += torch.sum(preds == target.data).item()\n",
    "            test_samples[attr] += target.size(0)\n",
    "\n",
    "# Print final test accuracy for each attribute\n",
    "for attr in attribute_columns_men_tshirts:\n",
    "    test_acc = test_corrects[attr] / test_samples[attr] if test_samples[attr] > 0 else 0\n",
    "    print(f\"Final Test Accuracy for Attribute {attr}: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:24:45.774429Z",
     "iopub.status.busy": "2024-11-10T13:24:45.773486Z",
     "iopub.status.idle": "2024-11-10T13:24:47.995403Z",
     "shell.execute_reply": "2024-11-10T13:24:47.994538Z",
     "shell.execute_reply.started": "2024-11-10T13:24:45.774382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_vit_men_tshirts.state_dict(), 'model_vit_men_tshirts2.pth')\n",
    "torch.save(model_vit_women_tops.state_dict(), 'model_vit_women_tops2.pth')\n",
    "torch.save(model_vit_women_tshirts.state_dict(), 'model_vit_women_tshirts2.pth')\n",
    "torch.save(model_vit_kurtis.state_dict(), 'model_vit_kurtis2.pth')\n",
    "torch.save(model_vit_sarees.state_dict(), 'model_vit_sarees2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:24:58.524005Z",
     "iopub.status.busy": "2024-11-10T13:24:58.523079Z",
     "iopub.status.idle": "2024-11-10T13:24:58.531774Z",
     "shell.execute_reply": "2024-11-10T13:24:58.530844Z",
     "shell.execute_reply.started": "2024-11-10T13:24:58.523961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(image_path, model, category_mappings, processor):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs[\"pixel_values\"].to(device))\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    # Process the outputs for each attribute\n",
    "    for attr, output in outputs.items():\n",
    "        probabilities = torch.softmax(output, dim=-1).cpu().numpy()\n",
    "        predicted_class_index = np.argmax(probabilities, axis=-1)[0]\n",
    "        \n",
    "        # Check if index exists in category mapping\n",
    "        if predicted_class_index in category_mappings[attr]:\n",
    "            predictions[attr] = category_mappings[attr][predicted_class_index]\n",
    "        else:\n",
    "            # Handle unmapped indices\n",
    "            print(category_mappings, attr, predicted_class_index)\n",
    "            predictions[attr] = 'unknown'  # or any default value you prefer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:25:03.256585Z",
     "iopub.status.busy": "2024-11-10T13:25:03.254954Z",
     "iopub.status.idle": "2024-11-10T13:37:51.229284Z",
     "shell.execute_reply": "2024-11-10T13:37:51.228309Z",
     "shell.execute_reply.started": "2024-11-10T13:25:03.256538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 30205/30205 [12:47<00:00, 39.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV generated and saved as 'final_predictions_vit2.csv'.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "from IPython.display import HTML\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "categories_number = {\n",
    "    'Men Tshirts': 5, \n",
    "    'Sarees': 10, \n",
    "    'Kurtis': 9, \n",
    "    'Women Tshirts': 8, \n",
    "    'Women Tops & Tunics': 10\n",
    "}\n",
    "\n",
    "model_selection = {\n",
    "    'Men Tshirts': model_vit_men_tshirts,\n",
    "    'Sarees': model_vit_sarees,\n",
    "    'Kurtis': model_vit_kurtis,\n",
    "    'Women Tshirts': model_vit_women_tshirts,\n",
    "    'Women Tops & Tunics': model_vit_women_tops\n",
    "}\n",
    "\n",
    "mapping_selection = {\n",
    "    'Men Tshirts': category_mappings_men_tshirts,\n",
    "    'Sarees': category_mappings_sarees,\n",
    "    'Kurtis': category_mappings_kurtis,\n",
    "    'Women Tshirts': category_mappings_women_tshirts,\n",
    "    'Women Tops & Tunics': category_mappings_women_tops\n",
    "}\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Images\"):\n",
    "    category = row['Category']\n",
    "    \n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        image_path = '/kaggle/input/visual-taxonomy/test_images/' + \"{:06}\".format(row['id']) + '.jpg'\n",
    "        predictions = predict(image_path, model_selection[category], mapping_selection[category], processor)\n",
    "    \n",
    "    result = {\n",
    "        'id': row['id'],\n",
    "        'Category': category,\n",
    "        'len': categories_number[category],  \n",
    "        **predictions  \n",
    "    }\n",
    "    \n",
    "    final_results.append(result)\n",
    "\n",
    "final_df = pd.DataFrame(final_results)\n",
    "\n",
    "final_df.to_csv('final_predictions_vit2.csv', index=False)\n",
    "\n",
    "print(\"Final CSV generated and saved as 'final_predictions_vit2.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting for train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:16:09.333412Z",
     "iopub.status.busy": "2024-11-13T06:16:09.332602Z",
     "iopub.status.idle": "2024-11-13T06:16:09.343393Z",
     "shell.execute_reply": "2024-11-13T06:16:09.342259Z",
     "shell.execute_reply.started": "2024-11-13T06:16:09.333358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(image_path, model, category_mappings, processor):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs[\"pixel_values\"].to(device))\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    # Process the outputs for each attribute\n",
    "    for attr, output in outputs.items():\n",
    "        probabilities = torch.softmax(output, dim=-1).cpu().numpy()\n",
    "        predicted_class_index = np.argmax(probabilities, axis=-1)[0]\n",
    "        \n",
    "        # Check if index exists in category mapping\n",
    "        if predicted_class_index in category_mappings[attr]:\n",
    "            predictions[attr] = category_mappings[attr][predicted_class_index]\n",
    "        else:\n",
    "            # Handle unmapped indices\n",
    "            print(category_mappings, attr, predicted_class_index)\n",
    "            predictions[attr] = 'unknown'  # or any default value you prefer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:20:39.336662Z",
     "iopub.status.busy": "2024-11-13T06:20:39.335657Z",
     "iopub.status.idle": "2024-11-13T06:20:39.356326Z",
     "shell.execute_reply": "2024-11-13T06:20:39.355161Z",
     "shell.execute_reply.started": "2024-11-13T06:20:39.336611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "category_mappings_men_tshirts = {'attr_1': {0: 'black', 1: 'default', 2: 'multicolor', 3: 'white'}, 'attr_2': {0: 'polo', 1: 'round'}, 'attr_3': {0: 'printed', 1: 'solid'}, 'attr_4': {0: 'default', 1: 'solid', 2: 'typography'}, 'attr_5': {0: 'long sleeves', 1: 'short sleeves'}, 'attr_6': {0: 'dummy_value'}, 'attr_7': {0: 'dummy_value'}, 'attr_8': {0: 'dummy_value'}, 'attr_9': {0: 'dummy_value'}, 'attr_10': {0: 'dummy_value'}}\n",
    "category_mappings_sarees = {'attr_1': {0: 'default', 1: 'same as border', 2: 'same as saree', 3: 'solid'}, 'attr_2': {0: 'default', 1: 'no border', 2: 'solid', 3: 'temple border', 4: 'woven design', 5: 'zari'}, 'attr_3': {0: 'big border', 1: 'no border', 2: 'small border'}, 'attr_4': {0: 'cream', 1: 'default', 2: 'green', 3: 'multicolor', 4: 'navy blue', 5: 'pink', 6: 'white', 7: 'yellow'}, 'attr_5': {0: 'daily', 1: 'party', 2: 'traditional', 3: 'wedding'}, 'attr_6': {0: 'default', 1: 'jacquard', 2: 'tassels and latkans'}, 'attr_7': {0: 'default', 1: 'same as saree', 2: 'woven design', 3: 'zari woven'}, 'attr_8': {0: 'default', 1: 'printed', 2: 'solid', 3: 'woven design', 4: 'zari woven'}, 'attr_9': {0: 'applique', 1: 'botanical', 2: 'checked', 3: 'default', 4: 'elephant', 5: 'ethnic motif', 6: 'floral', 7: 'peacock', 8: 'solid'}, 'attr_10': {0: 'no', 1: 'yes'}}\n",
    "category_mappings_kurtis = {'attr_1': {0: 'black', 1: 'blue', 2: 'green', 3: 'grey', 4: 'maroon', 5: 'multicolor', 6: 'navy blue', 7: 'orange', 8: 'pink', 9: 'purple', 10: 'red', 11: 'white', 12: 'yellow'}, 'attr_2': {0: 'a-line', 1: 'straight'}, 'attr_3': {0: 'calf length', 1: 'knee length'}, 'attr_4': {0: 'daily', 1: 'party'}, 'attr_5': {0: 'default', 1: 'net'}, 'attr_6': {0: 'default', 1: 'solid'}, 'attr_7': {0: 'default', 1: 'solid'}, 'attr_8': {0: 'short sleeves', 1: 'sleeveless', 2: 'three-quarter sleeves'}, 'attr_9': {0: 'regular', 1: 'sleeveless'}, 'attr_10': {0: 'dummy_value'}}\n",
    "category_mappings_women_tshirts = {'attr_1': {0: 'black', 1: 'default', 2: 'maroon', 3: 'multicolor', 4: 'pink', 5: 'white', 6: 'yellow'}, 'attr_2': {0: 'boxy', 1: 'loose', 2: 'regular'}, 'attr_3': {0: 'crop', 1: 'long', 2: 'regular'}, 'attr_4': {0: 'default', 1: 'printed', 2: 'solid'}, 'attr_5': {0: 'default', 1: 'funky print', 2: 'graphic', 3: 'quirky', 4: 'solid', 5: 'typography'}, 'attr_6': {0: 'default', 1: 'long sleeves', 2: 'short sleeves'}, 'attr_7': {0: 'cuffed sleeves', 1: 'regular sleeves'}, 'attr_8': {0: 'applique', 1: 'default'}, 'attr_9': {0: 'dummy_value'}, 'attr_10': {0: 'dummy_value'}}\n",
    "category_mappings_women_tops = {'attr_1': {0: 'black', 1: 'blue', 2: 'default', 3: 'green', 4: 'maroon', 5: 'multicolor', 6: 'navy blue', 7: 'peach', 8: 'pink', 9: 'red', 10: 'white', 11: 'yellow'}, 'attr_2': {0: 'boxy', 1: 'default', 2: 'fitted', 3: 'regular'}, 'attr_3': {0: 'crop', 1: 'regular'}, 'attr_4': {0: 'default', 1: 'high', 2: 'round neck', 3: 'square neck', 4: 'stylised', 5: 'sweetheart neck', 6: 'v-neck'}, 'attr_5': {0: 'casual', 1: 'party'}, 'attr_6': {0: 'default', 1: 'printed', 2: 'solid'}, 'attr_7': {0: 'default', 1: 'floral', 2: 'graphic', 3: 'quirky', 4: 'solid', 5: 'typography'}, 'attr_8': {0: 'long sleeves', 1: 'short sleeves', 2: 'sleeveless', 3: 'three-quarter sleeves'}, 'attr_9': {0: 'default', 1: 'puff sleeves', 2: 'regular sleeves', 3: 'sleeveless'}, 'attr_10': {0: 'applique', 1: 'default', 2: 'knitted', 3: 'ruffles', 4: 'tie-ups', 5: 'waist tie-ups'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:23:48.960019Z",
     "iopub.status.busy": "2024-11-13T06:23:48.959622Z",
     "iopub.status.idle": "2024-11-13T06:23:48.967105Z",
     "shell.execute_reply": "2024-11-13T06:23:48.966104Z",
     "shell.execute_reply.started": "2024-11-13T06:23:48.959981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_per_attribute):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.heads = nn.ModuleDict({\n",
    "            attr: nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "            for attr, num_classes in num_classes_per_attribute.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.base_model(pixel_values=pixel_values)\n",
    "        cls_hidden_states = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = {attr: self.heads[attr](cls_hidden_states) for attr in self.heads}\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:24:11.033547Z",
     "iopub.status.busy": "2024-11-13T06:24:11.033185Z",
     "iopub.status.idle": "2024-11-13T06:24:13.579769Z",
     "shell.execute_reply": "2024-11-13T06:24:13.578875Z",
     "shell.execute_reply.started": "2024-11-13T06:24:11.033511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70229dd7c44448e8fbd11823c0da45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2237f68c54242798438c093f397c2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a398a821a77441658eb51909ad0cfb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model\n",
    "base_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:24:59.263902Z",
     "iopub.status.busy": "2024-11-13T06:24:59.263510Z",
     "iopub.status.idle": "2024-11-13T06:25:00.145565Z",
     "shell.execute_reply": "2024-11-13T06:25:00.144609Z",
     "shell.execute_reply.started": "2024-11-13T06:24:59.263865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3368246996.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_men.load_state_dict(torch.load('/kaggle/working/model_vit_men_tshirts2.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputModel(\n",
       "  (base_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (attr_1): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_2): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_3): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_4): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_5): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_6): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (attr_7): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (attr_8): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (attr_9): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (attr_10): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_per_attribute_men_tshirts = {\n",
    "    'attr_1': 4,\n",
    "    'attr_2': 2,\n",
    "    'attr_3': 2,\n",
    "    'attr_4': 3,\n",
    "    'attr_5': 3,\n",
    "    'attr_6': 1,\n",
    "    'attr_7': 1,\n",
    "    'attr_8': 1,\n",
    "    'attr_9': 1,\n",
    "    'attr_10': 1\n",
    "}\n",
    "\n",
    "# Create an instance of the MultiOutputModel\n",
    "model_men = MultiOutputModel(base_model, num_classes_per_attribute_men_tshirts)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model_men.load_state_dict(torch.load('/kaggle/working/model_vit_men_tshirts2.pth'))\n",
    "model_men.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:25:32.915564Z",
     "iopub.status.busy": "2024-11-13T06:25:32.914785Z",
     "iopub.status.idle": "2024-11-13T06:25:33.162224Z",
     "shell.execute_reply": "2024-11-13T06:25:33.161241Z",
     "shell.execute_reply.started": "2024-11-13T06:25:32.915520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2540298325.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_kurtis.load_state_dict(torch.load('/kaggle/working/model_vit_kurtis2.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputModel(\n",
       "  (base_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (attr_1): Linear(in_features=768, out_features=13, bias=True)\n",
       "    (attr_2): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_3): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_4): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_5): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_6): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_7): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_8): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_9): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_10): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_per_attribute_kurtis = {\n",
    "    'attr_1': 13,\n",
    "    'attr_2': 2,\n",
    "    'attr_3': 2,\n",
    "    'attr_4': 2,\n",
    "    'attr_5': 2,\n",
    "    'attr_6': 2,\n",
    "    'attr_7': 2,\n",
    "    'attr_8': 3,\n",
    "    'attr_9': 2,\n",
    "    'attr_10': 1\n",
    "}\n",
    "\n",
    "# Create an instance of the MultiOutputModel\n",
    "model_kurtis = MultiOutputModel(base_model, num_classes_per_attribute_kurtis)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model_kurtis.load_state_dict(torch.load('/kaggle/working/model_vit_kurtis2.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model_kurtis.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:26:05.187621Z",
     "iopub.status.busy": "2024-11-13T06:26:05.187007Z",
     "iopub.status.idle": "2024-11-13T06:26:05.438185Z",
     "shell.execute_reply": "2024-11-13T06:26:05.437195Z",
     "shell.execute_reply.started": "2024-11-13T06:26:05.187578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1460995409.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_women_tops_tunics.load_state_dict(torch.load('/kaggle/working/model_vit_women_tops2.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputModel(\n",
       "  (base_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (attr_1): Linear(in_features=768, out_features=12, bias=True)\n",
       "    (attr_2): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_3): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_4): Linear(in_features=768, out_features=7, bias=True)\n",
       "    (attr_5): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_6): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_7): Linear(in_features=768, out_features=6, bias=True)\n",
       "    (attr_8): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_9): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_10): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_per_attribute_women_tops = {\n",
    "    'attr_1': 12,\n",
    "    'attr_2': 4,\n",
    "    'attr_3': 2,\n",
    "    'attr_4': 7,\n",
    "    'attr_5': 2,\n",
    "    'attr_6': 3,\n",
    "    'attr_7': 6,\n",
    "    'attr_8': 4,\n",
    "    'attr_9': 4,\n",
    "    'attr_10': 6\n",
    "}\n",
    "\n",
    "# Create an instance of the MultiOutputModel\n",
    "model_women_tops_tunics = MultiOutputModel(base_model, num_classes_per_attribute_women_tops)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model_women_tops_tunics.load_state_dict(torch.load('/kaggle/working/model_vit_women_tops2.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model_women_tops_tunics.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:26:30.713633Z",
     "iopub.status.busy": "2024-11-13T06:26:30.713257Z",
     "iopub.status.idle": "2024-11-13T06:26:30.940459Z",
     "shell.execute_reply": "2024-11-13T06:26:30.939254Z",
     "shell.execute_reply.started": "2024-11-13T06:26:30.713595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/609821489.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_women_tshirts.load_state_dict(torch.load('/kaggle/working/model_vit_women_tshirts2.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputModel(\n",
       "  (base_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (attr_1): Linear(in_features=768, out_features=7, bias=True)\n",
       "    (attr_2): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_3): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_4): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_5): Linear(in_features=768, out_features=6, bias=True)\n",
       "    (attr_6): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_7): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_8): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (attr_9): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (attr_10): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_per_attribute_women_tshirts = {\n",
    "    'attr_1': 7,\n",
    "    'attr_2': 3,\n",
    "    'attr_3': 3,\n",
    "    'attr_4': 3,\n",
    "    'attr_5': 6,\n",
    "    'attr_6': 3,\n",
    "    'attr_7': 2,\n",
    "    'attr_8': 2,\n",
    "    'attr_9': 1,\n",
    "    'attr_10': 1\n",
    "}\n",
    "\n",
    "# Create an instance of the MultiOutputModel\n",
    "model_women_tshirts = MultiOutputModel(base_model, num_classes_per_attribute_women_tshirts)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model_women_tshirts.load_state_dict(torch.load('/kaggle/working/model_vit_women_tshirts2.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model_women_tshirts.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:26:56.833323Z",
     "iopub.status.busy": "2024-11-13T06:26:56.832589Z",
     "iopub.status.idle": "2024-11-13T06:26:57.067578Z",
     "shell.execute_reply": "2024-11-13T06:26:57.066673Z",
     "shell.execute_reply.started": "2024-11-13T06:26:56.833281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2351436169.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_sarees.load_state_dict(torch.load('/kaggle/working/model_vit_sarees2.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputModel(\n",
       "  (base_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (attr_1): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_2): Linear(in_features=768, out_features=6, bias=True)\n",
       "    (attr_3): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_4): Linear(in_features=768, out_features=8, bias=True)\n",
       "    (attr_5): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_6): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (attr_7): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (attr_8): Linear(in_features=768, out_features=5, bias=True)\n",
       "    (attr_9): Linear(in_features=768, out_features=9, bias=True)\n",
       "    (attr_10): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_per_attribute_sarees = {\n",
    "    'attr_1': 4,  \n",
    "    'attr_2': 6,\n",
    "    'attr_3': 3,\n",
    "    'attr_4': 8,\n",
    "    'attr_5': 4,\n",
    "    'attr_6': 3,\n",
    "    'attr_7': 4,\n",
    "    'attr_8': 5,\n",
    "    'attr_9': 9,\n",
    "    'attr_10': 2\n",
    "}\n",
    "\n",
    "# Create an instance of the MultiOutputModel\n",
    "model_sarees = MultiOutputModel(base_model, num_classes_per_attribute_sarees)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model_sarees.load_state_dict(torch.load('/kaggle/working/model_vit_sarees2.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model_sarees.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T06:28:39.591314Z",
     "iopub.status.busy": "2024-11-13T06:28:39.590618Z",
     "iopub.status.idle": "2024-11-13T07:01:00.547250Z",
     "shell.execute_reply": "2024-11-13T07:01:00.546248Z",
     "shell.execute_reply.started": "2024-11-13T06:28:39.591272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 70213/70213 [32:20<00:00, 36.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV generated and saved as 'predictions_vit2_train.csv'.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "from IPython.display import HTML\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Mapping of categories to their respective 'lens' values\n",
    "categories_number = {\n",
    "    'Men Tshirts': 5, \n",
    "    'Sarees': 10, \n",
    "    'Kurtis': 9, \n",
    "    'Women Tshirts': 8, \n",
    "    'Women Tops & Tunics': 10\n",
    "}\n",
    "\n",
    "model_selection = {\n",
    "    'Men Tshirts': model_men,\n",
    "    'Sarees': model_sarees,\n",
    "    'Kurtis': model_kurtis,\n",
    "    'Women Tshirts': model_women_tshirts,\n",
    "    'Women Tops & Tunics': model_women_tops_tunics\n",
    "}\n",
    "\n",
    "mapping_selection = {\n",
    "    'Men Tshirts': category_mappings_men_tshirts,\n",
    "    'Sarees': category_mappings_sarees,\n",
    "    'Kurtis': category_mappings_kurtis,\n",
    "    'Women Tshirts': category_mappings_women_tshirts,\n",
    "    'Women Tops & Tunics': category_mappings_women_tops\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the final results\n",
    "final_results = []\n",
    "\n",
    "# Iterate over the train DataFrame with tqdm for a single progress bar\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing Images\"):\n",
    "    category = row['Category']\n",
    "    \n",
    "    # Suppress output by redirecting stdout temporarily\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        image_path = '/kaggle/input/visual-taxonomy/train_images/' + \"{:06}\".format(row['id']) + '.jpg'\n",
    "        predictions = predict(image_path, model_selection[category], mapping_selection[category], processor)\n",
    "    \n",
    "    # Extract attributes from the predictions\n",
    "    result = {\n",
    "        'id': row['id'],\n",
    "        'Category': category,\n",
    "        'len': categories_number[category],  # Set 'len' according to the category\n",
    "        **predictions  # Unpack the dictionary of attributes into the result\n",
    "    }\n",
    "    \n",
    "    final_results.append(result)\n",
    "\n",
    "# Create the final DataFrame from the results\n",
    "final_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv('predictions_vit2_train.csv', index=False)\n",
    "\n",
    "print(\"Final CSV generated and saved as 'predictions_vit2_train.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9755748,
     "sourceId": 84705,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
